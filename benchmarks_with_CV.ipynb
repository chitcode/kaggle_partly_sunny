{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Trying with different estimators with Cross Validation.\n",
      "Out of these different estimators few may be useful"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "\n",
      "\"\"\"Porter Stemming Algorithm\n",
      "This is the Porter stemming algorithm, ported to Python from the\n",
      "version coded up in ANSI C by the author. It may be be regarded\n",
      "as canonical, in that it follows the algorithm presented in\n",
      "\n",
      "Porter, 1980, An algorithm for suffix stripping, Program, Vol. 14,\n",
      "no. 3, pp 130-137,\n",
      "\n",
      "only differing from it at the points maked --DEPARTURE-- below.\n",
      "\n",
      "See also http://www.tartarus.org/~martin/PorterStemmer\n",
      "\n",
      "The algorithm as described in the paper could be exactly replicated\n",
      "by adjusting the points of DEPARTURE, but this is barely necessary,\n",
      "because (a) the points of DEPARTURE are definitely improvements, and\n",
      "(b) no encoding of the Porter stemmer I have seen is anything like\n",
      "as exact as this version, even with the points of DEPARTURE!\n",
      "\n",
      "Vivake Gupta (v@nano.com)\n",
      "\n",
      "Release 1: January 2001\n",
      "\n",
      "Further adjustments by Santiago Bruno (bananabruno@gmail.com)\n",
      "to allow word input not restricted to one word per line, leading\n",
      "to:\n",
      "\n",
      "release 2: July 2008\n",
      "\"\"\"\n",
      "\n",
      "import sys\n",
      "\n",
      "class PorterStemmer:\n",
      "\n",
      "    def __init__(self):\n",
      "        \"\"\"The main part of the stemming algorithm starts here.\n",
      "        b is a buffer holding a word to be stemmed. The letters are in b[k0],\n",
      "        b[k0+1] ... ending at b[k]. In fact k0 = 0 in this demo program. k is\n",
      "        readjusted downwards as the stemming progresses. Zero termination is\n",
      "        not in fact used in the algorithm.\n",
      "\n",
      "        Note that only lower case sequences are stemmed. Forcing to lower case\n",
      "        should be done before stem(...) is called.\n",
      "        \"\"\"\n",
      "\n",
      "        self.b = \"\"  # buffer for word to be stemmed\n",
      "        self.k = 0\n",
      "        self.k0 = 0\n",
      "        self.j = 0   # j is a general offset into the string\n",
      "\n",
      "    def cons(self, i):\n",
      "        \"\"\"cons(i) is TRUE <=> b[i] is a consonant.\"\"\"\n",
      "        if self.b[i] == 'a' or self.b[i] == 'e' or self.b[i] == 'i' or self.b[i] == 'o' or self.b[i] == 'u':\n",
      "            return 0\n",
      "        if self.b[i] == 'y':\n",
      "            if i == self.k0:\n",
      "                return 1\n",
      "            else:\n",
      "                return (not self.cons(i - 1))\n",
      "        return 1\n",
      "\n",
      "    def m(self):\n",
      "        \"\"\"m() measures the number of consonant sequences between k0 and j.\n",
      "        if c is a consonant sequence and v a vowel sequence, and <..>\n",
      "        indicates arbitrary presence,\n",
      "\n",
      "           <c><v>       gives 0\n",
      "           <c>vc<v>     gives 1\n",
      "           <c>vcvc<v>   gives 2\n",
      "           <c>vcvcvc<v> gives 3\n",
      "           ....\n",
      "        \"\"\"\n",
      "        n = 0\n",
      "        i = self.k0\n",
      "        while 1:\n",
      "            if i > self.j:\n",
      "                return n\n",
      "            if not self.cons(i):\n",
      "                break\n",
      "            i = i + 1\n",
      "        i = i + 1\n",
      "        while 1:\n",
      "            while 1:\n",
      "                if i > self.j:\n",
      "                    return n\n",
      "                if self.cons(i):\n",
      "                    break\n",
      "                i = i + 1\n",
      "            i = i + 1\n",
      "            n = n + 1\n",
      "            while 1:\n",
      "                if i > self.j:\n",
      "                    return n\n",
      "                if not self.cons(i):\n",
      "                    break\n",
      "                i = i + 1\n",
      "            i = i + 1\n",
      "\n",
      "    def vowelinstem(self):\n",
      "        \"\"\"vowelinstem() is TRUE <=> k0,...j contains a vowel\"\"\"\n",
      "        for i in range(self.k0, self.j + 1):\n",
      "            if not self.cons(i):\n",
      "                return 1\n",
      "        return 0\n",
      "\n",
      "    def doublec(self, j):\n",
      "        \"\"\"doublec(j) is TRUE <=> j,(j-1) contain a double consonant.\"\"\"\n",
      "        if j < (self.k0 + 1):\n",
      "            return 0\n",
      "        if (self.b[j] != self.b[j-1]):\n",
      "            return 0\n",
      "        return self.cons(j)\n",
      "\n",
      "    def cvc(self, i):\n",
      "        \"\"\"cvc(i) is TRUE <=> i-2,i-1,i has the form consonant - vowel - consonant\n",
      "        and also if the second c is not w,x or y. this is used when trying to\n",
      "        restore an e at the end of a short  e.g.\n",
      "\n",
      "           cav(e), lov(e), hop(e), crim(e), but\n",
      "           snow, box, tray.\n",
      "        \"\"\"\n",
      "        if i < (self.k0 + 2) or not self.cons(i) or self.cons(i-1) or not self.cons(i-2):\n",
      "            return 0\n",
      "        ch = self.b[i]\n",
      "        if ch == 'w' or ch == 'x' or ch == 'y':\n",
      "            return 0\n",
      "        return 1\n",
      "\n",
      "    def ends(self, s):\n",
      "        \"\"\"ends(s) is TRUE <=> k0,...k ends with the string s.\"\"\"\n",
      "        length = len(s)\n",
      "        if s[length - 1] != self.b[self.k]: # tiny speed-up\n",
      "            return 0\n",
      "        if length > (self.k - self.k0 + 1):\n",
      "            return 0\n",
      "        if self.b[self.k-length+1:self.k+1] != s:\n",
      "            return 0\n",
      "        self.j = self.k - length\n",
      "        return 1\n",
      "\n",
      "    def setto(self, s):\n",
      "        \"\"\"setto(s) sets (j+1),...k to the characters in the string s, readjusting k.\"\"\"\n",
      "        length = len(s)\n",
      "        self.b = self.b[:self.j+1] + s + self.b[self.j+length+1:]\n",
      "        self.k = self.j + length\n",
      "\n",
      "    def r(self, s):\n",
      "        \"\"\"r(s) is used further down.\"\"\"\n",
      "        if self.m() > 0:\n",
      "            self.setto(s)\n",
      "\n",
      "    def step1ab(self):\n",
      "        \"\"\"step1ab() gets rid of plurals and -ed or -ing. e.g.\n",
      "\n",
      "           caresses  ->  caress\n",
      "           ponies    ->  poni\n",
      "           ties      ->  ti\n",
      "           caress    ->  caress\n",
      "           cats      ->  cat\n",
      "\n",
      "           feed      ->  feed\n",
      "           agreed    ->  agree\n",
      "           disabled  ->  disable\n",
      "\n",
      "           matting   ->  mat\n",
      "           mating    ->  mate\n",
      "           meeting   ->  meet\n",
      "           milling   ->  mill\n",
      "           messing   ->  mess\n",
      "\n",
      "           meetings  ->  meet\n",
      "        \"\"\"\n",
      "        if self.b[self.k] == 's':\n",
      "            if self.ends(\"sses\"):\n",
      "                self.k = self.k - 2\n",
      "            elif self.ends(\"ies\"):\n",
      "                self.setto(\"i\")\n",
      "            elif self.b[self.k - 1] != 's':\n",
      "                self.k = self.k - 1\n",
      "        if self.ends(\"eed\"):\n",
      "            if self.m() > 0:\n",
      "                self.k = self.k - 1\n",
      "        elif (self.ends(\"ed\") or self.ends(\"ing\")) and self.vowelinstem():\n",
      "            self.k = self.j\n",
      "            if self.ends(\"at\"):   self.setto(\"ate\")\n",
      "            elif self.ends(\"bl\"): self.setto(\"ble\")\n",
      "            elif self.ends(\"iz\"): self.setto(\"ize\")\n",
      "            elif self.doublec(self.k):\n",
      "                self.k = self.k - 1\n",
      "                ch = self.b[self.k]\n",
      "                if ch == 'l' or ch == 's' or ch == 'z':\n",
      "                    self.k = self.k + 1\n",
      "            elif (self.m() == 1 and self.cvc(self.k)):\n",
      "                self.setto(\"e\")\n",
      "\n",
      "    def step1c(self):\n",
      "        \"\"\"step1c() turns terminal y to i when there is another vowel in the stem.\"\"\"\n",
      "        if (self.ends(\"y\") and self.vowelinstem()):\n",
      "            self.b = self.b[:self.k] + 'i' + self.b[self.k+1:]\n",
      "\n",
      "    def step2(self):\n",
      "        \"\"\"step2() maps double suffices to single ones.\n",
      "        so -ization ( = -ize plus -ation) maps to -ize etc. note that the\n",
      "        string before the suffix must give m() > 0.\n",
      "        \"\"\"\n",
      "        if self.b[self.k - 1] == 'a':\n",
      "            if self.ends(\"ational\"):   self.r(\"ate\")\n",
      "            elif self.ends(\"tional\"):  self.r(\"tion\")\n",
      "        elif self.b[self.k - 1] == 'c':\n",
      "            if self.ends(\"enci\"):      self.r(\"ence\")\n",
      "            elif self.ends(\"anci\"):    self.r(\"ance\")\n",
      "        elif self.b[self.k - 1] == 'e':\n",
      "            if self.ends(\"izer\"):      self.r(\"ize\")\n",
      "        elif self.b[self.k - 1] == 'l':\n",
      "            if self.ends(\"bli\"):       self.r(\"ble\") # --DEPARTURE--\n",
      "            # To match the published algorithm, replace this phrase with\n",
      "            #   if self.ends(\"abli\"):      self.r(\"able\")\n",
      "            elif self.ends(\"alli\"):    self.r(\"al\")\n",
      "            elif self.ends(\"entli\"):   self.r(\"ent\")\n",
      "            elif self.ends(\"eli\"):     self.r(\"e\")\n",
      "            elif self.ends(\"ousli\"):   self.r(\"ous\")\n",
      "        elif self.b[self.k - 1] == 'o':\n",
      "            if self.ends(\"ization\"):   self.r(\"ize\")\n",
      "            elif self.ends(\"ation\"):   self.r(\"ate\")\n",
      "            elif self.ends(\"ator\"):    self.r(\"ate\")\n",
      "        elif self.b[self.k - 1] == 's':\n",
      "            if self.ends(\"alism\"):     self.r(\"al\")\n",
      "            elif self.ends(\"iveness\"): self.r(\"ive\")\n",
      "            elif self.ends(\"fulness\"): self.r(\"ful\")\n",
      "            elif self.ends(\"ousness\"): self.r(\"ous\")\n",
      "        elif self.b[self.k - 1] == 't':\n",
      "            if self.ends(\"aliti\"):     self.r(\"al\")\n",
      "            elif self.ends(\"iviti\"):   self.r(\"ive\")\n",
      "            elif self.ends(\"biliti\"):  self.r(\"ble\")\n",
      "        elif self.b[self.k - 1] == 'g': # --DEPARTURE--\n",
      "            if self.ends(\"logi\"):      self.r(\"log\")\n",
      "        # To match the published algorithm, delete this phrase\n",
      "\n",
      "    def step3(self):\n",
      "        \"\"\"step3() dels with -ic-, -full, -ness etc. similar strategy to step2.\"\"\"\n",
      "        if self.b[self.k] == 'e':\n",
      "            if self.ends(\"icate\"):     self.r(\"ic\")\n",
      "            elif self.ends(\"ative\"):   self.r(\"\")\n",
      "            elif self.ends(\"alize\"):   self.r(\"al\")\n",
      "        elif self.b[self.k] == 'i':\n",
      "            if self.ends(\"iciti\"):     self.r(\"ic\")\n",
      "        elif self.b[self.k] == 'l':\n",
      "            if self.ends(\"ical\"):      self.r(\"ic\")\n",
      "            elif self.ends(\"ful\"):     self.r(\"\")\n",
      "        elif self.b[self.k] == 's':\n",
      "            if self.ends(\"ness\"):      self.r(\"\")\n",
      "\n",
      "    def step4(self):\n",
      "        \"\"\"step4() takes off -ant, -ence etc., in context <c>vcvc<v>.\"\"\"\n",
      "        if self.b[self.k - 1] == 'a':\n",
      "            if self.ends(\"al\"): pass\n",
      "            else: return\n",
      "        elif self.b[self.k - 1] == 'c':\n",
      "            if self.ends(\"ance\"): pass\n",
      "            elif self.ends(\"ence\"): pass\n",
      "            else: return\n",
      "        elif self.b[self.k - 1] == 'e':\n",
      "            if self.ends(\"er\"): pass\n",
      "            else: return\n",
      "        elif self.b[self.k - 1] == 'i':\n",
      "            if self.ends(\"ic\"): pass\n",
      "            else: return\n",
      "        elif self.b[self.k - 1] == 'l':\n",
      "            if self.ends(\"able\"): pass\n",
      "            elif self.ends(\"ible\"): pass\n",
      "            else: return\n",
      "        elif self.b[self.k - 1] == 'n':\n",
      "            if self.ends(\"ant\"): pass\n",
      "            elif self.ends(\"ement\"): pass\n",
      "            elif self.ends(\"ment\"): pass\n",
      "            elif self.ends(\"ent\"): pass\n",
      "            else: return\n",
      "        elif self.b[self.k - 1] == 'o':\n",
      "            if self.ends(\"ion\") and (self.b[self.j] == 's' or self.b[self.j] == 't'): pass\n",
      "            elif self.ends(\"ou\"): pass\n",
      "            # takes care of -ous\n",
      "            else: return\n",
      "        elif self.b[self.k - 1] == 's':\n",
      "            if self.ends(\"ism\"): pass\n",
      "            else: return\n",
      "        elif self.b[self.k - 1] == 't':\n",
      "            if self.ends(\"ate\"): pass\n",
      "            elif self.ends(\"iti\"): pass\n",
      "            else: return\n",
      "        elif self.b[self.k - 1] == 'u':\n",
      "            if self.ends(\"ous\"): pass\n",
      "            else: return\n",
      "        elif self.b[self.k - 1] == 'v':\n",
      "            if self.ends(\"ive\"): pass\n",
      "            else: return\n",
      "        elif self.b[self.k - 1] == 'z':\n",
      "            if self.ends(\"ize\"): pass\n",
      "            else: return\n",
      "        else:\n",
      "            return\n",
      "        if self.m() > 1:\n",
      "            self.k = self.j\n",
      "\n",
      "    def step5(self):\n",
      "        \"\"\"step5() removes a final -e if m() > 1, and changes -ll to -l if\n",
      "        m() > 1.\n",
      "        \"\"\"\n",
      "        self.j = self.k\n",
      "        if self.b[self.k] == 'e':\n",
      "            a = self.m()\n",
      "            if a > 1 or (a == 1 and not self.cvc(self.k-1)):\n",
      "                self.k = self.k - 1\n",
      "        if self.b[self.k] == 'l' and self.doublec(self.k) and self.m() > 1:\n",
      "            self.k = self.k -1\n",
      "\n",
      "    def stem(self, p, i, j):\n",
      "        \"\"\"In stem(p,i,j), p is a char pointer, and the string to be stemmed\n",
      "        is from p[i] to p[j] inclusive. Typically i is zero and j is the\n",
      "        offset to the last character of a string, (p[j+1] == '\\0'). The\n",
      "        stemmer adjusts the characters p[i] ... p[j] and returns the new\n",
      "        end-point of the string, k. Stemming never increases word length, so\n",
      "        i <= k <= j. To turn the stemmer into a module, declare 'stem' as\n",
      "        extern, and delete the remainder of this file.\n",
      "        \"\"\"\n",
      "        # copy the parameters into statics\n",
      "        self.b = p\n",
      "        self.k = j\n",
      "        self.k0 = i\n",
      "        if self.k <= self.k0 + 1:\n",
      "            return self.b # --DEPARTURE--\n",
      "\n",
      "        # With this line, strings of length 1 or 2 don't go through the\n",
      "        # stemming process, although no mention is made of this in the\n",
      "        # published algorithm. Remove the line to match the published\n",
      "        # algorithm.\n",
      "\n",
      "        self.step1ab()\n",
      "        self.step1c()\n",
      "        self.step2()\n",
      "        self.step3()\n",
      "        self.step4()\n",
      "        self.step5()\n",
      "        return self.b[self.k0:self.k+1]\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    p = PorterStemmer()\n",
      "    word = 'raining'\n",
      "    print p.stem(word,0,len(word)-1)\n",
      "#    if len(sys.argv) > 1:\n",
      "#        for f in sys.argv[1:]:\n",
      "#            infile = open(f, 'r')\n",
      "#            while 1:\n",
      "#                output = ''\n",
      "#                word = ''\n",
      "#                line = infile.readline()\n",
      "#                if line == '':\n",
      "#                    break\n",
      "#                for c in line:\n",
      "#                    if c.isalpha():\n",
      "#                        word += c.lower()\n",
      "#                    else:\n",
      "#                        if word:\n",
      "#                            output += p.stem(word, 0,len(word)-1)\n",
      "#                            word = ''\n",
      "#                        output += c.lower()\n",
      "#                print output,\n",
      "#            infile.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "rain\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Author : Chitrasen\n",
      "#Date : 11/1/2013\n",
      "# Basic benchmark code LogisticRegression\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn import cross_validation,metrics\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import linear_model as lm\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import re\n",
      "#import PorterStemmer\n",
      "\n",
      "def clean(s):\n",
      "    s = s.lower()\n",
      "    s = re.sub('n\\'t',' not',s)\n",
      "    s = re.sub('{link}',' ',s)\n",
      "    s = re.sub('#\\w+',' ',s)\n",
      "    s = re.sub('@\\w+',' ',s)\n",
      "    s = re.sub('[\\W]',' ',s)\n",
      "    s = re.sub('\\s+',' ',s)\n",
      "    s = removeStopWords(s)\n",
      "    return s\n",
      "\n",
      "def removeStopWords(s):\n",
      "    stopWordsStr = 'a about above after again against all am an and any are as at be because been before being below between both but by cannot could did do does doing down during each few for from further had has have having he he\\'d he\\'ll he\\'s her here here\\'s hers herself him himself his how how\\'s i i\\'d i\\'ll i\\'m i\\'ve if in into is it it\\'s its itself let\\'s me more most my myself no nor not of off on once only or other ought our ours  ourselves out over own rt same she she\\'d she\\'ll she\\'s should so some such than that that\\'s the their theirs them themselves then there there\\'s these they they\\'d they\\'ll they\\'re they\\'ve this those through to too under until up very was we we\\'d we\\'ll we\\'re we\\'ve were what what\\'s when when\\'s where where\\'s which while who who\\'s whom why why\\'s with would you you\\'d you\\'ll you\\'re you\\'ve your yours yourself yourselves'\n",
      "    stopWords = re.split('\\s+',stopWordsStr)\n",
      "    cleanwords = [w for w in re.split('\\W+',s) if w not in stopWords]\n",
      "    \n",
      "    p = PorterStemmer()\n",
      "    \n",
      "    s = ''\n",
      "    for word in cleanwords:\n",
      "        s += \" \" +p.stem(word,0,len(word)-1)\n",
      "    return s\n",
      "\n",
      "def y_binary_p(y,p):\n",
      "    if y <= p:\n",
      "        return 0    \n",
      "    else:\n",
      "        return 1\n",
      "\n",
      "\n",
      "def main():\n",
      "    \n",
      "    print 'reading the datasets ....'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv', header = 0)\n",
      "\n",
      "    pred = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred.index = test['id']   \n",
      "    \n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "\n",
      "\n",
      "    tfidf = TfidfVectorizer(min_df=2,  max_features=None, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
      "    \n",
      "    lr = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, \n",
      "                           C=1, fit_intercept=True, intercept_scaling=1.0, \n",
      "                           class_weight=None, random_state=None)\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "    X_all_a = list(tweets_train)+list(tweets_test)\n",
      "    X_all = [clean(s) for s in X_all_a]\n",
      "\n",
      "    tfidf.fit(X_all)\n",
      "    X_all = tfidf.transform(X_all)\n",
      "    \n",
      "    X_train = X_all[:trainLen]\n",
      "    X_test = X_all[trainLen:]\n",
      "    \n",
      "    vec_y_binary_p = np.vectorize(y_binary_p)\n",
      "    \n",
      "    final_cv_error = 0\n",
      "    \n",
      "    optim_p = {}\n",
      "    \n",
      "    #optim_p = {'s1':0.3,'s2':0.4,'s3':0.4,'s4':0.4,'s5':0.4,'w1':0.6,'w2':0.4,'w3':0.4,'w4':0.3,'k1':0.3,'k2':0.3,'k3':0.3,'k4':0.4,'k5':0.5,'k6':0.1,'k7':0.5,'k8':0.3,'k9':0.3,'k10':0.3,'k11':0.1,'k12':0.4,'k13':0.3,'k14':0.1,'k15':0.5}\n",
      "    print 'Finding the optimum p ... with 10 fold CV'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    \n",
      "    for y_col in train.columns[4:]:\n",
      "        min_error = 100\n",
      "        for i in linspace(0.05,1.0,num = 20):\n",
      "            mse_measure = []\n",
      "            for train_idx,cv_idx in kf:\n",
      "                X_train_kf = X_train[train_idx,:]\n",
      "                X_cv_kf = X_train[cv_idx,:]\n",
      "                \n",
      "                y = train[y_col]\n",
      "                y_train_kf = y[train_idx]\n",
      "                y_cv_kf = y[cv_idx]\n",
      "                \n",
      "                y_train_kf = vec_y_binary_p(y_train_kf,i)\n",
      "                lr.fit(X_train_kf,y_train_kf)\n",
      "                               \n",
      "                mse_measure.append(mean_squared_error(y_cv_kf,lr.predict_proba(X_cv_kf)[:,1]))\n",
      "            current_error =  np.mean(mse_measure)\n",
      "            if current_error < min_error:\n",
      "                min_error = current_error\n",
      "                optim_p[y_col] = i\n",
      "            else:\n",
      "                print 'optimum p for ',y_col,optim_p[y_col]\n",
      "                break        \n",
      "    \n",
      "    print 'CV score with optimun p'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf = X_train[train_idx,:]\n",
      "        X_cv_kf = X_train[cv_idx,:]\n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:]\n",
      "        #creating a place holder of cv predictions\n",
      "        pred_cv = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        error_pred = 100\n",
      "        \n",
      "        for y_col in train.columns[4:]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "            y_train_kf = vec_y_binary_p(y_train_kf,optim_p[y_col])\n",
      "            \n",
      "            lr.fit(X_train_kf,y_train_kf)\n",
      "            pred_cv[y_col] = lr.predict_proba(X_cv_kf)[:,1]\n",
      "            \n",
      "        error = sqrt(mean_squared_error(y_cv_kf,pred_cv))\n",
      "        final_error.append(error)\n",
      "        print 'final error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)\n",
      "    \n",
      "    \n",
      "    for y_col in train.columns[4:]:\n",
      "        y = train[y_col]\n",
      "        y = vec_y_binary_p(y,optim_p[y_col])\n",
      "        \n",
      "        lr.fit( X_train, y)\n",
      "        #print lr.predict_proba(X_test)\n",
      "        pred[y_col] = lr.predict_proba(X_test)[:,1]\n",
      "        \n",
      "    pred.to_csv('benchmark_lr_optim_p.csv')\n",
      "    print 'submission file created'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datasets ....\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finding the optimum p ... with 10 fold CV"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s1 0.25\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s2 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s3 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s4 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s5 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " w1 0.55\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " w2 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " w3 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " w4 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k1 0.25\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k2 0.25\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k3 0.25\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k4 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k5 0.45\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k6 0.1\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k7 0.5\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k8 0.25\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k9 0.35\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k10 0.25\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k11 0.05\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k12 0.35\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k13 0.25\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k14 0.1\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k15 0.45\n",
        "CV score with optimun p\n",
        "prcessing CV : 1\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.154536409688\n",
        "prcessing CV : 2\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.152337141013\n",
        "prcessing CV : 3\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.154198074314\n",
        "prcessing CV : 4\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 0.15481777107\n",
        "prcessing CV : 5\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 0.154389368997\n",
        "prcessing CV : 6\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 0.154740118197\n",
        "prcessing CV : 7\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 0.154442775252\n",
        "prcessing CV : 8\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 0.152742830375\n",
        "prcessing CV : 9\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 0.153214243042\n",
        "prcessing CV : 10\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 0.153523370034\n",
        "Mean error for 10 fold CV is  0.153894210198\n",
        "submission file created"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Author : Chitrasen\n",
      "#Date : 11/1/2013\n",
      "# TF (no idf) , best p value, \n",
      "\n",
      "import pandas as pd\n",
      "from sklearn import cross_validation,metrics\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import linear_model as lm\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import re\n",
      "#import PorterStemmer\n",
      "\n",
      "def clean(s):\n",
      "    s = s.lower()\n",
      "    s = re.sub('n\\'t',' not',s)\n",
      "    s = re.sub('{link}',' ',s)\n",
      "    s = re.sub('#\\w+',' ',s)\n",
      "    s = re.sub('@\\w+',' ',s)\n",
      "    s = re.sub('[\\W]',' ',s)\n",
      "    s = re.sub('\\s+',' ',s)\n",
      "    s = removeStopWords(s)\n",
      "    return s\n",
      "\n",
      "def removeStopWords(s):\n",
      "    stopWordsStr = 'a about above after again against all am an and any are as at be because been before being below between both but by cannot could did do does doing down during each few for from further had has have having he he\\'d he\\'ll he\\'s her here here\\'s hers herself him himself his how how\\'s i i\\'d i\\'ll i\\'m i\\'ve if in into is it it\\'s its itself let\\'s me more most my myself no nor not of off on once only or other ought our ours  ourselves out over own rt same she she\\'d she\\'ll she\\'s should so some such than that that\\'s the their theirs them themselves then there there\\'s these they they\\'d they\\'ll they\\'re they\\'ve this those through to too under until up very was we we\\'d we\\'ll we\\'re we\\'ve were what what\\'s when when\\'s where where\\'s which while who who\\'s whom why why\\'s with would you you\\'d you\\'ll you\\'re you\\'ve your yours yourself yourselves'\n",
      "    stopWords = re.split('\\s+',stopWordsStr)\n",
      "    cleanwords = [w for w in re.split('\\W+',s) if w not in stopWords]\n",
      "    \n",
      "    p = PorterStemmer()\n",
      "    \n",
      "    s = ''\n",
      "    for word in cleanwords:\n",
      "        s += \" \" +p.stem(word,0,len(word)-1)\n",
      "    return s\n",
      "\n",
      "def y_binary_p(y,p):\n",
      "    if y <= p:\n",
      "        return 0    \n",
      "    else:\n",
      "        return 1\n",
      "\n",
      "\n",
      "def main():\n",
      "    \n",
      "    print 'reading the datasets ....'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv', header = 0)\n",
      "\n",
      "    pred = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred.index = test['id']   \n",
      "    \n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "\n",
      "\n",
      "    tfidf = TfidfVectorizer(min_df=2,  max_features=None, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=False,sublinear_tf=1)\n",
      "    \n",
      "    lr = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, \n",
      "                           C=1, fit_intercept=True, intercept_scaling=1.0, \n",
      "                           class_weight=None, random_state=None)\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "    X_all_a = list(tweets_train)+list(tweets_test)\n",
      "    X_all = [clean(s) for s in X_all_a]\n",
      "\n",
      "    tfidf.fit(X_all)\n",
      "    X_all = tfidf.transform(X_all)\n",
      "    \n",
      "    X_train = X_all[:trainLen]\n",
      "    X_test = X_all[trainLen:]\n",
      "    \n",
      "    vec_y_binary_p = np.vectorize(y_binary_p)\n",
      "    \n",
      "    final_cv_error = 0\n",
      "    \n",
      "    optim_p = {}\n",
      "    \n",
      "    #optim_p = {'s1':0.3,'s2':0.4,'s3':0.4,'s4':0.4,'s5':0.4,'w1':0.6,'w2':0.4,'w3':0.4,'w4':0.3,'k1':0.3,'k2':0.3,'k3':0.3,'k4':0.4,'k5':0.5,'k6':0.1,'k7':0.5,'k8':0.3,'k9':0.3,'k10':0.3,'k11':0.1,'k12':0.4,'k13':0.3,'k14':0.1,'k15':0.5}\n",
      "    print 'Finding the optimum p ... with 10 fold CV'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    \n",
      "    for y_col in train.columns[4:]:\n",
      "        min_error = 100\n",
      "        for i in linspace(0.1,1.0,num = 10):\n",
      "            mse_measure = []\n",
      "            for train_idx,cv_idx in kf:\n",
      "                X_train_kf = X_train[train_idx,:]\n",
      "                X_cv_kf = X_train[cv_idx,:]\n",
      "                \n",
      "                y = train[y_col]\n",
      "                y_train_kf = y[train_idx]\n",
      "                y_cv_kf = y[cv_idx]\n",
      "                \n",
      "                y_train_kf = vec_y_binary_p(y_train_kf,i)\n",
      "                lr.fit(X_train_kf,y_train_kf)\n",
      "                               \n",
      "                mse_measure.append(mean_squared_error(y_cv_kf,lr.predict_proba(X_cv_kf)[:,1]))\n",
      "            current_error =  np.mean(mse_measure)\n",
      "            if current_error < min_error:\n",
      "                min_error = current_error\n",
      "                optim_p[y_col] = i\n",
      "            else:\n",
      "                print 'optimum p for ',y_col,optim_p[y_col]\n",
      "                break        \n",
      "    \n",
      "    print 'CV score with optimun p'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf = X_train[train_idx,:]\n",
      "        X_cv_kf = X_train[cv_idx,:]\n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:]\n",
      "        #creating a place holder of cv predictions\n",
      "        pred_cv = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        error_pred = 100\n",
      "        \n",
      "        for y_col in train.columns[4:]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "            y_train_kf = vec_y_binary_p(y_train_kf,optim_p[y_col])\n",
      "            \n",
      "            lr.fit(X_train_kf,y_train_kf)\n",
      "            pred_cv[y_col] = lr.predict_proba(X_cv_kf)[:,1]\n",
      "            \n",
      "        error = sqrt(mean_squared_error(y_cv_kf,pred_cv))\n",
      "        final_error.append(error)\n",
      "        print 'final error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)\n",
      "    \n",
      "    \n",
      "    for y_col in train.columns[4:]:\n",
      "        y = train[y_col]\n",
      "        y = vec_y_binary_p(y,optim_p[y_col])\n",
      "        \n",
      "        lr.fit( X_train, y)\n",
      "        #print lr.predict_proba(X_test)\n",
      "        pred[y_col] = lr.predict_proba(X_test)[:,1]\n",
      "        \n",
      "    pred.to_csv('benchmark_lr_optim_p.csv')\n",
      "    print 'submission file created'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datasets ....\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finding the optimum p ... with 10 fold CV"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s1 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s2 0.5\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s3 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s4 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s5 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " w1 0.6\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " w2 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " w3 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " w4 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k1 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k2 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k3 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k4 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k5 0.5\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k6 0.1\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k7 0.5\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k8 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k9 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k10 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k11 0.1\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k12 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k13 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k14 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k15 0.5\n",
        "CV score with optimun p\n",
        "prcessing CV : 1\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.153584138633\n",
        "prcessing CV : 2\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.151591946292\n",
        "prcessing CV : 3\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.153590092253\n",
        "prcessing CV : 4\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 0.154181430978\n",
        "prcessing CV : 5\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 0.153617703716\n",
        "prcessing CV : 6\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 0.154110280161\n",
        "prcessing CV : 7\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 0.154018744638\n",
        "prcessing CV : 8\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 0.152115571289\n",
        "prcessing CV : 9\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 0.152232972008\n",
        "prcessing CV : 10\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 0.152970422642\n",
        "Mean error for 10 fold CV is  0.153201330261\n",
        "submission file created"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Author : Chitrasen\n",
      "#Date : 11/1/2013\n",
      "# Basic benchmark code RidgeClassifier\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn import cross_validation,metrics\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.linear_model import RidgeClassifier\n",
      "\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import re\n",
      "#import PorterStemmer\n",
      "\n",
      "def clean(s):\n",
      "    s = s.lower()\n",
      "    s = re.sub('n\\'t',' not',s)\n",
      "    s = re.sub('{link}',' ',s)\n",
      "    s = re.sub('#\\w+',' ',s)\n",
      "    s = re.sub('@\\w+',' ',s)\n",
      "    s = re.sub('[\\W]',' ',s)\n",
      "    s = re.sub('\\s+',' ',s)\n",
      "    s = removeStopWords(s)\n",
      "    return s\n",
      "\n",
      "def removeStopWords(s):\n",
      "    stopWordsStr = 'a about above after again against all am an and any are as at be because been before being below between both but by cannot could did do does doing down during each few for from further had has have having he he\\'d he\\'ll he\\'s her here here\\'s hers herself him himself his how how\\'s i i\\'d i\\'ll i\\'m i\\'ve if in into is it it\\'s its itself let\\'s me more most my myself no nor not of off on once only or other ought our ours  ourselves out over own rt same she she\\'d she\\'ll she\\'s should so some such than that that\\'s the their theirs them themselves then there there\\'s these they they\\'d they\\'ll they\\'re they\\'ve this those through to too under until up very was we we\\'d we\\'ll we\\'re we\\'ve were what what\\'s when when\\'s where where\\'s which while who who\\'s whom why why\\'s with would you you\\'d you\\'ll you\\'re you\\'ve your yours yourself yourselves'\n",
      "    stopWords = re.split('\\s+',stopWordsStr)\n",
      "    cleanwords = [w for w in re.split('\\W+',s) if w not in stopWords]\n",
      "    \n",
      "    p = PorterStemmer()\n",
      "    \n",
      "    s = ''\n",
      "    for word in cleanwords:\n",
      "        s += \" \" +p.stem(word,0,len(word)-1)\n",
      "    return s\n",
      "\n",
      "def y_binary_p(y,p):\n",
      "    if y <= p:\n",
      "        return 0    \n",
      "    else:\n",
      "        return 1\n",
      "\n",
      "\n",
      "def main():\n",
      "    \n",
      "    print 'reading the datasets ....'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv', header = 0)\n",
      "\n",
      "    pred = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred.index = test['id']   \n",
      "    \n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "\n",
      "\n",
      "    tfidf = TfidfVectorizer(min_df=2,  max_features=None, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
      "    \n",
      "    #lr = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, \n",
      "    #                       C=1, fit_intercept=True, intercept_scaling=1.0, \n",
      "    #                       class_weight=None, random_state=None)\n",
      "    \n",
      "    clf = RidgeClassifier(tol=1e-2, solver=\"lsqr\",fit_intercept = 1)\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "    X_all_a = list(tweets_train)+list(tweets_test)\n",
      "    X_all = [clean(s) for s in X_all_a]\n",
      "\n",
      "    tfidf.fit(X_all)\n",
      "    X_all = tfidf.transform(X_all)\n",
      "    \n",
      "    X_train = X_all[:trainLen]\n",
      "    X_test = X_all[trainLen:]\n",
      "    \n",
      "    vec_y_binary_p = np.vectorize(y_binary_p)\n",
      "    \n",
      "    final_cv_error = 0\n",
      "    \n",
      "    optim_p = {}\n",
      "    \n",
      "    optim_p = {'s1':0.3,'s2':0.4,'s3':0.4,'s4':0.4,'s5':0.4,'w1':0.6,'w2':0.4,'w3':0.4,'w4':0.3,'k1':0.3,'k2':0.3,'k3':0.3,'k4':0.4,'k5':0.5,'k6':0.1,'k7':0.5,'k8':0.3,'k9':0.3,'k10':0.3,'k11':0.1,'k12':0.4,'k13':0.3,'k14':0.1,'k15':0.5}\n",
      "    #print 'Finding the optimum p ... with 10 fold CV'\n",
      "    \n",
      "    #kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    \n",
      "    #for y_col in train.columns[4:]:\n",
      "    #    min_error = 100\n",
      "    #    for i in linspace(0.05,1.0,num = 20):\n",
      "    #        mse_measure = []\n",
      "    #        for train_idx,cv_idx in kf:\n",
      "    #            X_train_kf = X_train[train_idx,:]\n",
      "    #            X_cv_kf = X_train[cv_idx,:]\n",
      "    #            \n",
      "    #            y = train[y_col]\n",
      "    #           y_train_kf = y[train_idx]\n",
      "    #            y_cv_kf = y[cv_idx]\n",
      "    #            \n",
      "    #            y_train_kf = vec_y_binary_p(y_train_kf,i)\n",
      "    #            lr.fit(X_train_kf,y_train_kf)\n",
      "    #                           \n",
      "    #            mse_measure.append(mean_squared_error(y_cv_kf,lr.predict_proba(X_cv_kf)[:,1]))\n",
      "    #        current_error =  np.mean(mse_measure)\n",
      "    #        if current_error < min_error:\n",
      "    #            min_error = current_error\n",
      "    #            optim_p[y_col] = i\n",
      "    #        else:\n",
      "    #            print 'optimum p for ',y_col,optim_p[y_col]\n",
      "    #            break        \n",
      "    \n",
      "    #print 'CV score with optimun p'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf = X_train[train_idx,:]\n",
      "        X_cv_kf = X_train[cv_idx,:]\n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:]\n",
      "        #creating a place holder of cv predictions\n",
      "        pred_cv = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        error_pred = 100\n",
      "        \n",
      "        for y_col in train.columns[4:]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "            y_train_kf = vec_y_binary_p(y_train_kf,optim_p[y_col])\n",
      "            \n",
      "            clf.fit(X_train_kf,y_train_kf.values)\n",
      "            pred_cv[y_col] = clf.predict(X_cv_kf)\n",
      "            \n",
      "        error = sqrt(mean_squared_error(y_cv_kf,pred_cv))\n",
      "        final_error.append(error)\n",
      "        print 'final error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)\n",
      "    \n",
      "    \n",
      "    for y_col in train.columns[4:]:\n",
      "        y = train[y_col]\n",
      "        y = vec_y_binary_p(y,optim_p[y_col])\n",
      "        \n",
      "        clf.fit( X_train, y)\n",
      "        #print lr.predict_proba(X_test)\n",
      "        pred[y_col] = clf.predict(X_test)\n",
      "        \n",
      "    pred.to_csv('benchmark_svc_optim_p.csv')\n",
      "    print 'submission file created'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datasets ....\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "prcessing CV :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.20986496757\n",
        "prcessing CV : 2\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.207688056239\n",
        "prcessing CV : 3\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.210564371547\n",
        "prcessing CV : 4\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 0.209604928159\n",
        "prcessing CV : 5\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 0.210610154686\n",
        "prcessing CV : 6\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 0.210443156312\n",
        "prcessing CV : 7\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 0.208914692649\n",
        "prcessing CV : 8\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 0.207539386695\n",
        "prcessing CV : 9\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 0.208662913656\n",
        "prcessing CV : 10\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 0.209006088537\n",
        "Mean error for 10 fold CV is  0.209289871605\n",
        "submission file created"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Author : Chitrasen\n",
      "#Date : 11/1/2013\n",
      "# Basic benchmark code SVM\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn import cross_validation,metrics\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.linear_model import RidgeClassifier\n",
      "\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import re\n",
      "#import PorterStemmer\n",
      "\n",
      "def clean(s):\n",
      "    s = s.lower()\n",
      "    s = re.sub('n\\'t',' not',s)\n",
      "    s = re.sub('{link}',' ',s)\n",
      "    s = re.sub('#\\w+',' ',s)\n",
      "    s = re.sub('@\\w+',' ',s)\n",
      "    s = re.sub('[\\W]',' ',s)\n",
      "    s = re.sub('\\s+',' ',s)\n",
      "    s = removeStopWords(s)\n",
      "    return s\n",
      "\n",
      "def removeStopWords(s):\n",
      "    stopWordsStr = 'a about above after again against all am an and any are as at be because been before being below between both but by cannot could did do does doing down during each few for from further had has have having he he\\'d he\\'ll he\\'s her here here\\'s hers herself him himself his how how\\'s i i\\'d i\\'ll i\\'m i\\'ve if in into is it it\\'s its itself let\\'s me more most my myself no nor not of off on once only or other ought our ours  ourselves out over own rt same she she\\'d she\\'ll she\\'s should so some such than that that\\'s the their theirs them themselves then there there\\'s these they they\\'d they\\'ll they\\'re they\\'ve this those through to too under until up very was we we\\'d we\\'ll we\\'re we\\'ve were what what\\'s when when\\'s where where\\'s which while who who\\'s whom why why\\'s with would you you\\'d you\\'ll you\\'re you\\'ve your yours yourself yourselves'\n",
      "    stopWords = re.split('\\s+',stopWordsStr)\n",
      "    cleanwords = [w for w in re.split('\\W+',s) if w not in stopWords]\n",
      "    \n",
      "    p = PorterStemmer()\n",
      "    \n",
      "    s = ''\n",
      "    for word in cleanwords:\n",
      "        s += \" \" +p.stem(word,0,len(word)-1)\n",
      "    return s\n",
      "\n",
      "def y_binary_p(y,p):\n",
      "    if y <= p:\n",
      "        return 0    \n",
      "    else:\n",
      "        return 1\n",
      "\n",
      "\n",
      "def main():\n",
      "    \n",
      "    print 'reading the datasets ....'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv', header = 0)\n",
      "\n",
      "    pred = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred.index = test['id']   \n",
      "    \n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "\n",
      "\n",
      "    tfidf = TfidfVectorizer(min_df=2,  max_features=None, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
      "    \n",
      "    #lr = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, \n",
      "    #                       C=1, fit_intercept=True, intercept_scaling=1.0, \n",
      "    #                       class_weight=None, random_state=None)\n",
      "    \n",
      "    clf = SVC(probability = True, verbose = True)\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "    X_all_a = list(tweets_train)+list(tweets_test)\n",
      "    X_all = [clean(s) for s in X_all_a]\n",
      "\n",
      "    tfidf.fit(X_all)\n",
      "    X_all = tfidf.transform(X_all)\n",
      "    \n",
      "    X_train = X_all[:trainLen]\n",
      "    X_test = X_all[trainLen:]\n",
      "    \n",
      "    vec_y_binary_p = np.vectorize(y_binary_p)\n",
      "    \n",
      "    final_cv_error = 0\n",
      "    \n",
      "    optim_p = {}\n",
      "    \n",
      "    optim_p = {'s1':0.3,'s2':0.4,'s3':0.4,'s4':0.4,'s5':0.4,'w1':0.6,'w2':0.4,'w3':0.4,'w4':0.3,'k1':0.3,'k2':0.3,'k3':0.3,'k4':0.4,'k5':0.5,'k6':0.1,'k7':0.5,'k8':0.3,'k9':0.3,'k10':0.3,'k11':0.1,'k12':0.4,'k13':0.3,'k14':0.1,'k15':0.5}\n",
      "    #print 'Finding the optimum p ... with 10 fold CV'\n",
      "    \n",
      "    #kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    \n",
      "    #for y_col in train.columns[4:]:\n",
      "    #    min_error = 100\n",
      "    #    for i in linspace(0.05,1.0,num = 20):\n",
      "    #        mse_measure = []\n",
      "    #        for train_idx,cv_idx in kf:\n",
      "    #            X_train_kf = X_train[train_idx,:]\n",
      "    #            X_cv_kf = X_train[cv_idx,:]\n",
      "    #            \n",
      "    #            y = train[y_col]\n",
      "    #           y_train_kf = y[train_idx]\n",
      "    #            y_cv_kf = y[cv_idx]\n",
      "    #            \n",
      "    #            y_train_kf = vec_y_binary_p(y_train_kf,i)\n",
      "    #            lr.fit(X_train_kf,y_train_kf)\n",
      "    #                           \n",
      "    #            mse_measure.append(mean_squared_error(y_cv_kf,lr.predict_proba(X_cv_kf)[:,1]))\n",
      "    #        current_error =  np.mean(mse_measure)\n",
      "    #        if current_error < min_error:\n",
      "    #            min_error = current_error\n",
      "    #            optim_p[y_col] = i\n",
      "    #        else:\n",
      "    #            print 'optimum p for ',y_col,optim_p[y_col]\n",
      "    #            break        \n",
      "    \n",
      "    #print 'CV score with optimun p'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf = X_train[train_idx,:]\n",
      "        X_cv_kf = X_train[cv_idx,:]\n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:]\n",
      "        #creating a place holder of cv predictions\n",
      "        pred_cv = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        error_pred = 100\n",
      "        \n",
      "        for y_col in train.columns[4:]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "            y_train_kf = vec_y_binary_p(y_train_kf,optim_p[y_col])\n",
      "            \n",
      "            clf.fit(X_train_kf,y_train_kf.values)\n",
      "            pred_cv[y_col] = clf.predict_proba(X_cv_kf)[:,1]\n",
      "            \n",
      "        error = sqrt(mean_squared_error(y_cv_kf,pred_cv))\n",
      "        final_error.append(error)\n",
      "        print 'final error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)\n",
      "    \n",
      "    \n",
      "    for y_col in train.columns[4:]:\n",
      "        y = train[y_col]\n",
      "        y = vec_y_binary_p(y,optim_p[y_col])\n",
      "        \n",
      "        clf.fit( X_train, y)\n",
      "        #print lr.predict_proba(X_test)\n",
      "        pred[y_col] = clf.predict_proba(X_test)[:,1]\n",
      "        \n",
      "    pred.to_csv('benchmark_svc_optim_p.csv')\n",
      "    print 'submission file created'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datasets ....\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "prcessing CV :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.198251133758\n",
        "prcessing CV : 2\n",
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Author : Chitrasen\n",
      "#Date : 11/1/2013\n",
      "# Basic benchmark code SGDClassifier\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn import cross_validation,metrics\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import re\n",
      "#import PorterStemmer\n",
      "\n",
      "def clean(s):\n",
      "    s = s.lower()\n",
      "    s = re.sub('n\\'t',' not',s)\n",
      "    s = re.sub('{link}',' ',s)\n",
      "    s = re.sub('#\\w+',' ',s)\n",
      "    s = re.sub('@\\w+',' ',s)\n",
      "    s = re.sub('[\\W]',' ',s)\n",
      "    s = re.sub('\\s+',' ',s)\n",
      "    s = removeStopWords(s)\n",
      "    return s\n",
      "\n",
      "def removeStopWords(s):\n",
      "    stopWordsStr = 'a about above after again against all am an and any are as at be because been before being below between both but by cannot could did do does doing down during each few for from further had has have having he he\\'d he\\'ll he\\'s her here here\\'s hers herself him himself his how how\\'s i i\\'d i\\'ll i\\'m i\\'ve if in into is it it\\'s its itself let\\'s me more most my myself no nor not of off on once only or other ought our ours  ourselves out over own rt same she she\\'d she\\'ll she\\'s should so some such than that that\\'s the their theirs them themselves then there there\\'s these they they\\'d they\\'ll they\\'re they\\'ve this those through to too under until up very was we we\\'d we\\'ll we\\'re we\\'ve were what what\\'s when when\\'s where where\\'s which while who who\\'s whom why why\\'s with would you you\\'d you\\'ll you\\'re you\\'ve your yours yourself yourselves'\n",
      "    stopWords = re.split('\\s+',stopWordsStr)\n",
      "    cleanwords = [w for w in re.split('\\W+',s) if w not in stopWords]\n",
      "    \n",
      "    p = PorterStemmer()\n",
      "    \n",
      "    s = ''\n",
      "    for word in cleanwords:\n",
      "        s += \" \" +p.stem(word,0,len(word)-1)\n",
      "    return s\n",
      "\n",
      "def y_binary_p(y,p):\n",
      "    if y <= p:\n",
      "        return 0    \n",
      "    else:\n",
      "        return 1\n",
      "\n",
      "\n",
      "def main():\n",
      "    \n",
      "    print 'reading the datasets ....'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv', header = 0)\n",
      "\n",
      "    pred = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred.index = test['id']   \n",
      "    \n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "\n",
      "\n",
      "    tfidf = TfidfVectorizer(min_df=2,  max_features=None, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
      "    \n",
      "    #lr = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, \n",
      "    #                       C=1, fit_intercept=True, intercept_scaling=1.0, \n",
      "    #                       class_weight=None, random_state=None)\n",
      "    \n",
      "    clf = SGDClassifier(n_iter = 50, n_jobs = 3, loss='modified_huber')\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "    X_all_a = list(tweets_train)+list(tweets_test)\n",
      "    X_all = [clean(s) for s in X_all_a]\n",
      "\n",
      "    tfidf.fit(X_all)\n",
      "    X_all = tfidf.transform(X_all)\n",
      "    \n",
      "    X_train = X_all[:trainLen]\n",
      "    X_test = X_all[trainLen:]\n",
      "    \n",
      "    vec_y_binary_p = np.vectorize(y_binary_p)\n",
      "    \n",
      "    final_cv_error = 0\n",
      "    \n",
      "    optim_p = {}\n",
      "    \n",
      "    optim_p = {'s1':0.3,'s2':0.4,'s3':0.4,'s4':0.4,'s5':0.4,'w1':0.6,'w2':0.4,'w3':0.4,'w4':0.3,'k1':0.3,'k2':0.3,'k3':0.3,'k4':0.4,'k5':0.5,'k6':0.1,'k7':0.5,'k8':0.3,'k9':0.3,'k10':0.3,'k11':0.1,'k12':0.4,'k13':0.3,'k14':0.1,'k15':0.5}\n",
      "    #print 'Finding the optimum p ... with 10 fold CV'\n",
      "    \n",
      "    #kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    \n",
      "    #for y_col in train.columns[4:]:\n",
      "    #    min_error = 100\n",
      "    #    for i in linspace(0.05,1.0,num = 20):\n",
      "    #        mse_measure = []\n",
      "    #        for train_idx,cv_idx in kf:\n",
      "    #            X_train_kf = X_train[train_idx,:]\n",
      "    #            X_cv_kf = X_train[cv_idx,:]\n",
      "    #            \n",
      "    #            y = train[y_col]\n",
      "    #           y_train_kf = y[train_idx]\n",
      "    #            y_cv_kf = y[cv_idx]\n",
      "    #            \n",
      "    #            y_train_kf = vec_y_binary_p(y_train_kf,i)\n",
      "    #            lr.fit(X_train_kf,y_train_kf)\n",
      "    #                           \n",
      "    #            mse_measure.append(mean_squared_error(y_cv_kf,lr.predict_proba(X_cv_kf)[:,1]))\n",
      "    #        current_error =  np.mean(mse_measure)\n",
      "    #        if current_error < min_error:\n",
      "    #            min_error = current_error\n",
      "    #            optim_p[y_col] = i\n",
      "    #        else:\n",
      "    #            print 'optimum p for ',y_col,optim_p[y_col]\n",
      "    #            break        \n",
      "    \n",
      "    #print 'CV score with optimun p'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf = X_train[train_idx,:]\n",
      "        X_cv_kf = X_train[cv_idx,:]\n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:]\n",
      "        #creating a place holder of cv predictions\n",
      "        pred_cv = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        error_pred = 100\n",
      "        \n",
      "        for y_col in train.columns[4:]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "            y_train_kf = vec_y_binary_p(y_train_kf,optim_p[y_col])\n",
      "            \n",
      "            clf.fit(X_train_kf,y_train_kf.values)\n",
      "            pred_cv[y_col] = clf.predict_proba(X_cv_kf)[:,1]\n",
      "            \n",
      "        error = sqrt(mean_squared_error(y_cv_kf,pred_cv))\n",
      "        final_error.append(error)\n",
      "        print 'final error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)\n",
      "    \n",
      "    \n",
      "    for y_col in train.columns[4:]:\n",
      "        y = train[y_col]\n",
      "        y = vec_y_binary_p(y,optim_p[y_col])\n",
      "        \n",
      "        clf.fit( X_train, y.values)\n",
      "        #print lr.predict_proba(X_test)\n",
      "        pred[y_col] = clf.predict_proba(X_test)[:,1]\n",
      "        \n",
      "    pred.to_csv('benchmark_svc_optim_p.csv')\n",
      "    print 'submission file created'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datasets ....\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "prcessing CV :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.15445179096\n",
        "prcessing CV : 2\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.15251036359\n",
        "prcessing CV : 3\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.154472562222\n",
        "prcessing CV : 4\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 0.154769271479\n",
        "prcessing CV : 5\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 0.154408578604\n",
        "prcessing CV : 6\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 0.154824348233\n",
        "prcessing CV : 7\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 0.154551208252\n",
        "prcessing CV : 8\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 0.1525492229\n",
        "prcessing CV : 9\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 0.153377488602\n",
        "prcessing CV : 10\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 0.153821284348\n",
        "Mean error for 10 fold CV is  0.153973611919\n",
        "submission file created"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Author : Chitrasen\n",
      "#Date : 11/1/2013\n",
      "# Basic benchmark code - ElasticNet\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn import cross_validation,metrics\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import linear_model as lm\n",
      "\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import re\n",
      "#import PorterStemmer\n",
      "\n",
      "def clean(s):\n",
      "    s = s.lower()\n",
      "    s = re.sub('n\\'t',' not',s)\n",
      "    s = re.sub('{link}',' ',s)\n",
      "    s = re.sub('#\\w+',' ',s)\n",
      "    s = re.sub('@\\w+',' ',s)\n",
      "    s = re.sub('[\\W]',' ',s)\n",
      "    s = re.sub('\\s+',' ',s)\n",
      "    s = removeStopWords(s)\n",
      "    return s\n",
      "\n",
      "def removeStopWords(s):\n",
      "    stopWordsStr = 'a about above after again against all am an and any are as at be because been before being below between both but by cannot could did do does doing down during each few for from further had has have having he he\\'d he\\'ll he\\'s her here here\\'s hers herself him himself his how how\\'s i i\\'d i\\'ll i\\'m i\\'ve if in into is it it\\'s its itself let\\'s me more most my myself no nor not of off on once only or other ought our ours  ourselves out over own rt same she she\\'d she\\'ll she\\'s should so some such than that that\\'s the their theirs them themselves then there there\\'s these they they\\'d they\\'ll they\\'re they\\'ve this those through to too under until up very was we we\\'d we\\'ll we\\'re we\\'ve were what what\\'s when when\\'s where where\\'s which while who who\\'s whom why why\\'s with would you you\\'d you\\'ll you\\'re you\\'ve your yours yourself yourselves'\n",
      "    stopWords = re.split('\\s+',stopWordsStr)\n",
      "    cleanwords = [w for w in re.split('\\W+',s) if w not in stopWords]\n",
      "    \n",
      "    p = PorterStemmer()\n",
      "    \n",
      "    s = ''\n",
      "    for word in cleanwords:\n",
      "        s += \" \" +p.stem(word,0,len(word)-1)\n",
      "    return s\n",
      "\n",
      "\n",
      "def main():\n",
      "    \n",
      "    print 'reading the datasets ....'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv', header = 0)\n",
      "\n",
      "    pred = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred.index = test['id']   \n",
      "    \n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "\n",
      "\n",
      "    tfidf = TfidfVectorizer(min_df=2,  max_features=None, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
      "    \n",
      "    #lr = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, \n",
      "    #                       C=1, fit_intercept=True, intercept_scaling=1.0, \n",
      "    #                       class_weight=None, random_state=None)\n",
      "    \n",
      "    clf = lm.ElasticNet(alpha=.5, l1_ratio=0.7)\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "    X_all_a = list(tweets_train)+list(tweets_test)\n",
      "    X_all = [clean(s) for s in X_all_a]\n",
      "\n",
      "    tfidf.fit(X_all)\n",
      "    X_all = tfidf.transform(X_all)\n",
      "    \n",
      "    X_train = X_all[:trainLen]\n",
      "    X_test = X_all[trainLen:]\n",
      "    \n",
      "    vec_y_binary_p = np.vectorize(y_binary_p)\n",
      "    \n",
      "    final_cv_error = 0\n",
      "    \n",
      "    #optim_p = {}\n",
      "    \n",
      "    #optim_p = {'s1':0.3,'s2':0.4,'s3':0.4,'s4':0.4,'s5':0.4,'w1':0.6,'w2':0.4,'w3':0.4,'w4':0.3,'k1':0.3,'k2':0.3,'k3':0.3,'k4':0.4,'k5':0.5,'k6':0.1,'k7':0.5,'k8':0.3,'k9':0.3,'k10':0.3,'k11':0.1,'k12':0.4,'k13':0.3,'k14':0.1,'k15':0.5}\n",
      "    \n",
      "    \n",
      "    #print 'Finding the optimum p ... with 10 fold CV'\n",
      "    \n",
      "    #kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    \n",
      "    #for y_col in train.columns[4:]:\n",
      "    #    min_error = 100\n",
      "    #    for i in linspace(0.05,1.0,num = 20):\n",
      "    #        mse_measure = []\n",
      "    #        for train_idx,cv_idx in kf:\n",
      "    #            X_train_kf = X_train[train_idx,:]\n",
      "    #            X_cv_kf = X_train[cv_idx,:]\n",
      "    #            \n",
      "    #            y = train[y_col]\n",
      "    #           y_train_kf = y[train_idx]\n",
      "    #            y_cv_kf = y[cv_idx]\n",
      "    #            \n",
      "    #            y_train_kf = vec_y_binary_p(y_train_kf,i)\n",
      "    #            lr.fit(X_train_kf,y_train_kf)\n",
      "    #                           \n",
      "    #            mse_measure.append(mean_squared_error(y_cv_kf,lr.predict_proba(X_cv_kf)[:,1]))\n",
      "    #        current_error =  np.mean(mse_measure)\n",
      "    #        if current_error < min_error:\n",
      "    #            min_error = current_error\n",
      "    #            optim_p[y_col] = i\n",
      "    #        else:\n",
      "    #            print 'optimum p for ',y_col,optim_p[y_col]\n",
      "    #            break        \n",
      "    \n",
      "    #print 'CV score with optimun p'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf = X_train[train_idx,:]\n",
      "        X_cv_kf = X_train[cv_idx,:]\n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:]\n",
      "        #creating a place holder of cv predictions\n",
      "        pred_cv = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        error_pred = 100\n",
      "        \n",
      "        for y_col in train.columns[4:]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "            \n",
      "            \n",
      "            clf.fit(X_train_kf,y_train_kf.values)\n",
      "            pred_cv[y_col] = clf.predict(X_cv_kf)\n",
      "            \n",
      "        error = sqrt(mean_squared_error(y_cv_kf,pred_cv))\n",
      "        final_error.append(error)\n",
      "        print 'final error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)\n",
      "    \n",
      "    \n",
      "    for y_col in train.columns[4:]:\n",
      "        y = train[y_col]\n",
      "                \n",
      "        clf.fit( X_train, y.values)\n",
      "        #print lr.predict_proba(X_test)\n",
      "        pred[y_col] = clf.predict(X_test)\n",
      "        \n",
      "    pred.to_csv('benchmark_svc_optim_p.csv')\n",
      "    print 'submission file created'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datasets ....\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "prcessing CV :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.251246150926\n",
        "prcessing CV : 2\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.249481700494\n",
        "prcessing CV : 3\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.249362034266\n",
        "prcessing CV : 4\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 0.250022257488\n",
        "prcessing CV : 5\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 0.249771964735\n",
        "prcessing CV : 6\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 0.249710707384\n",
        "prcessing CV : 7\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 0.250189189968\n",
        "prcessing CV : 8\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 0.249783430616\n",
        "prcessing CV : 9\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 0.249731277631\n",
        "prcessing CV : 10\n",
        "final error for cv"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'ElasticNet' object has no attribute 'predict_proba'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-9-224fc7e1aad8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-9-224fc7e1aad8>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[1;31m#print lr.predict_proba(X_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m         \u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_col\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'benchmark_svc_optim_p.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'ElasticNet' object has no attribute 'predict_proba'"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 0.249073839979\n",
        "Mean error for 10 fold CV is  0.249837255349\n"
       ]
      }
     ],
     "prompt_number": 9
    }
   ],
   "metadata": {}
  }
 ]
}