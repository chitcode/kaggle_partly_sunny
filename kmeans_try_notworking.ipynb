{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Trying with K-Means"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
      "import re\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn import cross_validation\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "\n",
      "def errorFunc(pred,actual):\n",
      "    #return np.mean(np.square(pred-actual).sum())\n",
      "    return mean_squared_error(pred,actual)\n",
      "\n",
      "def main():\n",
      "    print 'reading the datsets...'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    train['tweet'] = [re.sub('\\d+','00',tweet.lower()) for tweet in train.tweet]\n",
      "    \n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv',header = 0)\n",
      "    test['tweet'] = [re.sub('\\d+','00',tweet.lower()) for tweet in test.tweet]\n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "    X_all = list(tweets_train)+list(tweets_test)\n",
      "    \n",
      "    tfidf =  TfidfVectorizer(max_features=5000,strip_accents='unicode',  \n",
      "        analyzer='word',token_pattern=r'\\w{2,}',ngram_range=(1,1),stop_words = 'english', min_df = 0.2)\n",
      "    \n",
      "    lsa = TruncatedSVD(n_components= 100, n_iterations = 50)   \n",
      "    regg = KNeighborsRegressor(n_neighbors=2)\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "\n",
      "\n",
      "    tfidf.fit(X_all)\n",
      "    X_all = tfidf.transform(X_all)    \n",
      "\n",
      "    print 'Fitting in LSA'\n",
      "    #X_all = lsa.fit_transform(X_all)\n",
      "\n",
      "    X_train = X_all[:trainLen]\n",
      "    X_test = X_all[trainLen:] \n",
      "    \n",
      "    kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf = X_train[train_idx,:]\n",
      "        X_cv_kf = X_train[cv_idx,:]\n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:9]\n",
      "        #creating a place holder of cv predictions\n",
      "        pred_cv = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:9])\n",
      "        \n",
      "        error_pred = 100\n",
      "        \n",
      "        for y_col in train.columns[4:9]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "           # y_train_kf = vec_y_binary_p(y_train_kf,0.5)\n",
      "            print 'Fitting for column ',y_col\n",
      "            regg.fit(X_train_kf,y_train_kf)\n",
      "            pred_cv[y_col] = regg.predict(X_cv_kf)\n",
      "            \n",
      "        #error = sqrt(mean_squared_error(y_cv_kf,pred_cv))\n",
      "        error = sqrt(errorFunc(y_cv_kf,pred_cv))\n",
      "        final_error.append(error)\n",
      "        print 'error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datsets...\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fitting in LSA"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "prcessing CV : 1\n",
        "Fitting for column "
       ]
      }
     ],
     "prompt_number": "*"
    }
   ],
   "metadata": {}
  }
 ]
}