{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Final Submissions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#submitted . \n",
      "# CV Score # 0.149443240783\n",
      "#Public leader board score # 0.15610\n",
      "#Author : Chitrasen\n",
      "#Date : 11/1/2013\n",
      "# TF (no idf) , best p value, \n",
      "\n",
      "import pandas as pd\n",
      "from sklearn import cross_validation,metrics\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import linear_model as lm\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import re\n",
      "#import PorterStemmer\n",
      "\n",
      "def clean(s):\n",
      "    s = s.lower()\n",
      "    s = re.sub('n\\'t',' not',s)\n",
      "    s = re.sub('{link}',' ',s)\n",
      "    s = re.sub('#\\w+',' ',s)\n",
      "    s = re.sub('@\\w+',' ',s)\n",
      "    s = re.sub('[\\W]',' ',s)\n",
      "    s = re.sub('\\s+',' ',s)\n",
      "    s = removeStopWords(s)\n",
      "    return s\n",
      "\n",
      "def removeStopWords(s):\n",
      "    stopWordsStr = 'a about above after again against all am an and any are as at be because been before being below between both but by cannot could did do does doing down during each few for from further had has have having he he\\'d he\\'ll he\\'s her here here\\'s hers herself him himself his how how\\'s i i\\'d i\\'ll i\\'m i\\'ve if in into is it it\\'s its itself let\\'s me more most my myself no nor not of off on once only or other ought our ours  ourselves out over own rt same she she\\'d she\\'ll she\\'s should so some such than that that\\'s the their theirs them themselves then there there\\'s these they they\\'d they\\'ll they\\'re they\\'ve this those through to too under until up very was we we\\'d we\\'ll we\\'re we\\'ve were what what\\'s when when\\'s where where\\'s which while who who\\'s whom why why\\'s with would you you\\'d you\\'ll you\\'re you\\'ve your yours yourself yourselves'\n",
      "    stopWords = re.split('\\s+',stopWordsStr)\n",
      "    cleanwords = [w for w in re.split('\\W+',s) if w not in stopWords]\n",
      "    \n",
      "    p = PorterStemmer()\n",
      "    \n",
      "    s = ''\n",
      "    for word in cleanwords:\n",
      "        s += \" \" +p.stem(word,0,len(word)-1)\n",
      "    return s\n",
      "\n",
      "def y_binary_p(y,p):\n",
      "    if y <= p:\n",
      "        return 0    \n",
      "    else:\n",
      "        return 1\n",
      "\n",
      "\n",
      "def main():\n",
      "    \n",
      "    print 'reading the datasets ....'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv', header = 0)\n",
      "\n",
      "    pred = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred.index = test['id']   \n",
      "    \n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "\n",
      "\n",
      "    tfidf = TfidfVectorizer(min_df=5,  max_features=10000, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=False,sublinear_tf=1)\n",
      "    \n",
      "    lr = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, \n",
      "                           C=1, fit_intercept=True, intercept_scaling=1.0, \n",
      "                           class_weight=None, random_state=None)\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "    X_all = list(tweets_train)+list(tweets_test)\n",
      "    #X_all = [clean(s) for s in X_all_a]\n",
      "\n",
      "    tfidf.fit(X_all)\n",
      "    X_all = tfidf.transform(X_all)\n",
      "    \n",
      "    X_train = X_all[:trainLen]\n",
      "    X_test = X_all[trainLen:]\n",
      "    \n",
      "    vec_y_binary_p = np.vectorize(y_binary_p)\n",
      "    \n",
      "    final_cv_error = 0\n",
      "    \n",
      "    optim_p = {}\n",
      "    \n",
      "    #optim_p = {'s1':0.3,'s2':0.4,'s3':0.4,'s4':0.4,'s5':0.4,\n",
      "    #     'w1':0.6,'w2':0.4,'w3':0.4,'w4':0.3,\n",
      "    #     'k1':0.3,'k2':0.3,'k3':0.3,'k4':0.4,'k5':0.5,'k6':0.1,'k7':0.5,'k8':0.3,'k9':0.3,'k10':0.3,'k11':0.1,'k12':0.4,'k13':0.3,'k14':0.1,'k15':0.5}\n",
      "    print 'Finding the optimum p ... with 10 fold CV'\n",
      "    \n",
      "    kfs = cross_validation.ShuffleSplit(X_train.shape[0], n_iter=10,test_size=.25, random_state=0)\n",
      "    \n",
      "    \n",
      "    for y_col in train.columns[4:]:\n",
      "        min_error = 100\n",
      "        for i in linspace(0.1,1.0,num = 10):\n",
      "            mse_measure = []\n",
      "            for train_idx,cv_idx in kfs:\n",
      "                X_train_kf = X_train[train_idx,:]\n",
      "                X_cv_kf = X_train[cv_idx,:]\n",
      "                \n",
      "                y = train[y_col]\n",
      "                y_train_kf = y[train_idx]\n",
      "                y_cv_kf = y[cv_idx]\n",
      "                \n",
      "                y_train_kf = vec_y_binary_p(y_train_kf,i)\n",
      "                lr.fit(X_train_kf,y_train_kf)\n",
      "                               \n",
      "                mse_measure.append(mean_squared_error(y_cv_kf,lr.predict_proba(X_cv_kf)[:,1]))\n",
      "            current_error =  np.mean(mse_measure)\n",
      "            if current_error < min_error:\n",
      "                min_error = current_error\n",
      "                optim_p[y_col] = i\n",
      "            else:\n",
      "                print 'optimum p for ',y_col,optim_p[y_col]\n",
      "                break        \n",
      "    \n",
      "    print 'CV score with optimun p'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf = X_train[train_idx,:]\n",
      "        X_cv_kf = X_train[cv_idx,:]\n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:]\n",
      "        #creating a place holder of cv predictions\n",
      "        pred_cv = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        error_pred = 100\n",
      "        \n",
      "        for y_col in train.columns[4:]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "            y_train_kf = vec_y_binary_p(y_train_kf,optim_p[y_col])\n",
      "            \n",
      "            lr.fit(X_train_kf,y_train_kf)\n",
      "            pred_cv[y_col] = lr.predict_proba(X_cv_kf)[:,1]\n",
      "            \n",
      "        error = sqrt(mean_squared_error(y_cv_kf,pred_cv))\n",
      "        final_error.append(error)\n",
      "        print 'final error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)\n",
      "    \n",
      "    \n",
      "    for y_col in train.columns[4:]:\n",
      "        y = train[y_col]\n",
      "        y = vec_y_binary_p(y,optim_p[y_col])\n",
      "        \n",
      "        lr.fit( X_train, y)\n",
      "        #print lr.predict_proba(X_test)\n",
      "        pred[y_col] = lr.predict_proba(X_test)[:,1]\n",
      "        \n",
      "    pred.to_csv('benchmark_lr_optim_p.csv')\n",
      "    print 'submission file created'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datasets ....\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finding the optimum p ... with 10 fold CV"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s1 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s2 0.5\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s3 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s4 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " s5 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " w1 0.6\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " w2 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " w3 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " w4 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k1 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k2 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k3 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k4 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k5 0.5\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k6 0.1\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k7 0.5\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k8 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k9 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k10 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k11 0.1\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k12 0.4\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k13 0.3\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k14 0.1\n",
        "optimum p for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " k15 0.5\n",
        "CV score with optimun p\n",
        "prcessing CV : 1\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.149138986584\n",
        "prcessing CV : 2\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.148006871519\n",
        "prcessing CV : 3\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.150228340815\n",
        "prcessing CV : 4\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 0.150324904763\n",
        "prcessing CV : 5\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 0.149806852174\n",
        "prcessing CV : 6\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 0.150314337505\n",
        "prcessing CV : 7\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 0.149628373103\n",
        "prcessing CV : 8\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 0.149067206531\n",
        "prcessing CV : 9\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 0.148313420847\n",
        "prcessing CV : 10\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 0.149603113991\n",
        "Mean error for 10 fold CV is  0.149443240783\n",
        "submission file created"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#submitted . \n",
      "# CV Score # 0.148805764072\n",
      "#Public leader board score # 0.15621\n",
      "#Author : Chitrasen\n",
      "#Date : 11/1/2013\n",
      "# TF (no idf + with idf) , best p value, \n",
      "\n",
      "import pandas as pd\n",
      "from sklearn import cross_validation,metrics\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import linear_model as lm\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import re\n",
      "#import PorterStemmer\n",
      "\n",
      "def clean(s):\n",
      "    s = s.lower()\n",
      "    s = re.sub('n\\'t',' not',s)\n",
      "    #s = re.sub(r'\\W+', ' ', s)\n",
      "    #s = re.sub('\\s+', ' ', s)\n",
      "    #s = re.sub('\\d+','00',s)\n",
      "    return s\n",
      "\n",
      "def removeStopWords(s):\n",
      "    stopWordsStr = 'a about above after again against all am an and any are as at be because been before being below between both but by cannot could did do does doing down during each few for from further had has have having he he\\'d he\\'ll he\\'s her here here\\'s hers herself him himself his how how\\'s i i\\'d i\\'ll i\\'m i\\'ve if in into is it it\\'s its itself let\\'s me more most my myself no nor not of off on once only or other ought our ours  ourselves out over own rt same she she\\'d she\\'ll she\\'s should so some such than that that\\'s the their theirs them themselves then there there\\'s these they they\\'d they\\'ll they\\'re they\\'ve this those through to too under until up very was we we\\'d we\\'ll we\\'re we\\'ve were what what\\'s when when\\'s where where\\'s which while who who\\'s whom why why\\'s with would you you\\'d you\\'ll you\\'re you\\'ve your yours yourself yourselves'\n",
      "    stopWords = re.split('\\s+',stopWordsStr)\n",
      "    cleanwords = [w for w in re.split('\\W+',s) if w not in stopWords]\n",
      "    \n",
      "    p = PorterStemmer()\n",
      "    \n",
      "    s = ''\n",
      "    for word in cleanwords:\n",
      "        s += \" \" +p.stem(word,0,len(word)-1)\n",
      "    return s\n",
      "\n",
      "def y_binary_p(y,p):\n",
      "    if y <= p:\n",
      "        return 0    \n",
      "    else:\n",
      "        return 1\n",
      "\n",
      "\n",
      "def main():\n",
      "    \n",
      "    print 'reading the datasets ....'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv', header = 0)\n",
      "    \n",
      "    train['tweet'] = [tweet+' '+str(state) for tweet,state in zip(train.tweet,train.state)]\n",
      "    test['tweet'] = [tweet+' '+str(state) for tweet,state in zip(test.tweet,test.state)]\n",
      "\n",
      "    pred1 = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred1.index = test['id'] \n",
      "    \n",
      "    pred2 = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred2.index = test['id']   \n",
      "    \n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "\n",
      "\n",
      "    tfidf1 = TfidfVectorizer(min_df=5,max_df = .95,max_features=10000, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1,2), use_idf=False,sublinear_tf=1)\n",
      "    \n",
      "    tfidf2 = TfidfVectorizer(min_df=5,max_df = .95,max_features=10000, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1,2), use_idf=True,sublinear_tf=1)\n",
      "    \n",
      "    \n",
      "    lr = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, \n",
      "                           C=1, fit_intercept=True, intercept_scaling=1.0, \n",
      "                           class_weight=None, random_state=None)\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "    X_all = list(tweets_train)+list(tweets_test)\n",
      "    X_all = [clean(s) for s in X_all]\n",
      "\n",
      "    tfidf1.fit(X_all)\n",
      "    X_all1 = tfidf1.transform(X_all)\n",
      "    \n",
      "    tfidf2.fit(X_all)\n",
      "    X_all2 = tfidf2.transform(X_all)\n",
      "    \n",
      "    X_train1 = X_all1[:trainLen]\n",
      "    X_test1 = X_all1[trainLen:]\n",
      "    \n",
      "    X_train2 = X_all2[:trainLen]\n",
      "    X_test2 = X_all2[trainLen:]\n",
      "    \n",
      "    vec_y_binary_p = np.vectorize(y_binary_p)\n",
      "    \n",
      "    final_cv_error = 0\n",
      "    \n",
      "    #optim_p = {}\n",
      "   \n",
      "    \n",
      "    optim_p = {'s1':0.3,'s2':0.5,'s3':0.4,'s4':0.4,'s5':0.4,\n",
      "         'w1':0.6,'w2':0.4,'w3':0.4,'w4':0.4,\n",
      "         'k1':0.3,'k2':0.3,'k3':0.3,'k4':0.4,'k5':0.5,'k6':0.1,'k7':0.5,'k8':0.3,'k9':0.3,'k10':0.4,\n",
      "            'k11':0.1,'k12':0.4,'k13':0.3,'k14':0.1,'k15':0.5}\n",
      "    print 'Finding the optimum p ... with 10 fold CV'\n",
      "    \n",
      "    #kfs = cross_validation.ShuffleSplit(X_train.shape[0], n_iter=10,test_size=.25, random_state=0)\n",
      "    \n",
      "    \n",
      "    #for y_col in train.columns[4:]:\n",
      "    #    min_error = 100\n",
      "    #    for i in linspace(0.1,1.0,num = 10):\n",
      "    #        mse_measure = []\n",
      "    #        for train_idx,cv_idx in kfs:\n",
      "    #            X_train_kf = X_train[train_idx,:]\n",
      "    #            X_cv_kf = X_train[cv_idx,:]\n",
      "    #            \n",
      "    #            y = train[y_col]\n",
      "    #            y_train_kf = y[train_idx]\n",
      "    #            y_cv_kf = y[cv_idx]\n",
      "    #            \n",
      "    #            y_train_kf = vec_y_binary_p(y_train_kf,i)\n",
      "    #            lr.fit(X_train_kf,y_train_kf)\n",
      "    #                           \n",
      "    #            mse_measure.append(mean_squared_error(y_cv_kf,lr.predict_proba(X_cv_kf)[:,1]))\n",
      "    #        current_error =  np.mean(mse_measure)\n",
      "    #        if current_error < min_error:\n",
      "    #            min_error = current_error\n",
      "    #            optim_p[y_col] = i\n",
      "    #        else:\n",
      "    #            print 'optimum p for ',y_col,optim_p[y_col]\n",
      "    #            break        \n",
      "    \n",
      "    print 'CV score with optimun p'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train1.shape[0],n_folds = 10)\n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf1 = X_train1[train_idx,:]\n",
      "        X_train_kf2 = X_train2[train_idx,:]\n",
      "        \n",
      "        X_cv_kf1 = X_train1[cv_idx,:]\n",
      "        X_cv_kf2 = X_train2[cv_idx,:]\n",
      "        \n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:]\n",
      "        #creating a place holder of cv predictions\n",
      "        pred_cv1 = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        pred_cv2 = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        for y_col in train.columns[4:]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "            y_train_kf = vec_y_binary_p(y_train_kf,optim_p[y_col])\n",
      "            \n",
      "            lr.fit(X_train_kf1,y_train_kf)\n",
      "            pred_cv1[y_col] = lr.predict_proba(X_cv_kf1)[:,1]\n",
      "            \n",
      "            lr.fit(X_train_kf2,y_train_kf)\n",
      "            pred_cv2[y_col] = lr.predict_proba(X_cv_kf2)[:,1]        \n",
      "            #print '    error for',y_col,sqrt(mean_squared_error(y_cv_kf[y_col],(pred_cv1[y_col] + pred_cv2[y_col])/2))\n",
      "           \n",
      "        pred_cv =  (pred_cv1 + pred_cv2)/2  \n",
      "        error = sqrt(mean_squared_error(y_cv_kf,pred_cv))\n",
      "        final_error.append(error)\n",
      "        print 'final error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)\n",
      "    \n",
      "    \n",
      "    for y_col in train.columns[4:]:\n",
      "        y = train[y_col]\n",
      "        y = vec_y_binary_p(y,optim_p[y_col])\n",
      "        \n",
      "        lr.fit( X_train1, y)        \n",
      "        pred1[y_col] = lr.predict_proba(X_test1)[:,1]\n",
      "        \n",
      "        lr.fit( X_train2, y)        \n",
      "        pred2[y_col] = lr.predict_proba(X_test2)[:,1]\n",
      "        \n",
      "    pred = (pred1 + pred2)/2\n",
      "    pred.to_csv('benchmark_lr_ensemble_optim_p.csv')\n",
      "    print 'submission file created'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datasets ....\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finding the optimum p ... with 10 fold CV"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CV score with optimun p\n",
        "prcessing CV : 1\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.148488510157\n",
        "prcessing CV : 2\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.147340225695\n",
        "prcessing CV : 3\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.149546168336\n",
        "prcessing CV : 4\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 0.149691036606\n",
        "prcessing CV : 5\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 0.149386053995\n",
        "prcessing CV : 6\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 0.149653576591\n",
        "prcessing CV : 7\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 0.148941245421\n",
        "prcessing CV : 8\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 0.148446006026\n",
        "prcessing CV : 9\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 0.147624941693\n",
        "prcessing CV : 10\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 0.148939876199\n",
        "Mean error for 10 fold CV is  0.148805764072\n",
        "submission file created"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#submission not required\n",
      "#Author : Chitrasen\n",
      "#Date : 11/1/2013\n",
      "# TF (no idf) , best p value, \n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn import cross_validation,metrics\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import linear_model as lm\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import re\n",
      "#import PorterStemmer\n",
      "\n",
      "def clean(s):\n",
      "    s = s.lower()\n",
      "    s = re.sub('n\\'t',' not',s)\n",
      "    s = re.sub('{link}',' ',s)\n",
      "    s = re.sub('#\\w+',' ',s)\n",
      "    s = re.sub('@\\w+',' ',s)\n",
      "    s = re.sub('[\\W]',' ',s)\n",
      "    s = re.sub('\\s+',' ',s)\n",
      "    s = removeStopWords(s)\n",
      "    return s\n",
      "\n",
      "def removeStopWords(s):\n",
      "    stopWordsStr = 'a about above after again against all am an and any are as at be because been before being below between both but by cannot could did do does doing down during each few for from further had has have having he he\\'d he\\'ll he\\'s her here here\\'s hers herself him himself his how how\\'s i i\\'d i\\'ll i\\'m i\\'ve if in into is it it\\'s its itself let\\'s me more most my myself no nor not of off on once only or other ought our ours  ourselves out over own rt same she she\\'d she\\'ll she\\'s should so some such than that that\\'s the their theirs them themselves then there there\\'s these they they\\'d they\\'ll they\\'re they\\'ve this those through to too under until up very was we we\\'d we\\'ll we\\'re we\\'ve were what what\\'s when when\\'s where where\\'s which while who who\\'s whom why why\\'s with would you you\\'d you\\'ll you\\'re you\\'ve your yours yourself yourselves'\n",
      "    stopWords = re.split('\\s+',stopWordsStr)\n",
      "    cleanwords = [w for w in re.split('\\W+',s) if w not in stopWords]\n",
      "    \n",
      "    p = PorterStemmer()\n",
      "    \n",
      "    s = ''\n",
      "    for word in cleanwords:\n",
      "        s += \" \" +p.stem(word,0,len(word)-1)\n",
      "    return s\n",
      "\n",
      "def y_binary_p(y,p):\n",
      "    if y <= p:\n",
      "        return 0    \n",
      "    else:\n",
      "        return 1\n",
      "\n",
      "\n",
      "def main():\n",
      "    \n",
      "    print 'reading the datasets ....'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv', header = 0)\n",
      "    train['tweet'] = [tweet+' '+str(state) for tweet,state in zip(train.tweet,train.state)]\n",
      "    test['tweet'] = [tweet+' '+str(state) for tweet,state in zip(test.tweet,test.state)]\n",
      "\n",
      "    pred = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred.index = test['id']   \n",
      "    \n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "    \n",
      "    tfidf1 = TfidfVectorizer(min_df=5,  max_features=10000, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=False,sublinear_tf=1)\n",
      "\n",
      "\n",
      "    tfidf2 = TfidfVectorizer(min_df=5,  max_features=10000, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=True,sublinear_tf=1)\n",
      "    \n",
      "    clf = lm.SGDClassifier(loss='modified_huber',n_iter = 100)\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "    X_all = list(tweets_train)+list(tweets_test)\n",
      "    #X_all = [clean(s) for s in X_all_a]\n",
      "\n",
      "    tfidf1.fit(X_all)\n",
      "    X_all1 = tfidf1.transform(X_all)\n",
      "    \n",
      "    tfidf2.fit(X_all)\n",
      "    X_all2 = tfidf2.transform(X_all)\n",
      "    \n",
      "    X_train1 = X_all1[:trainLen]\n",
      "    X_test1 = X_all1[trainLen:]\n",
      "    \n",
      "    X_train2 = X_all2[:trainLen]\n",
      "    X_test2 = X_all2[trainLen:]\n",
      "    \n",
      "    vec_y_binary_p = np.vectorize(y_binary_p)\n",
      "    \n",
      "    final_cv_error = 0\n",
      "    \n",
      "    optim_p = {'s1':0.3,'s2':0.5,'s3':0.4,'s4':0.5,'s5':0.5,\n",
      "         'w1':0.5,'w2':0.4,'w3':0.4,'w4':0.4,\n",
      "         'k1':0.4,'k2':0.4,'k3':0.4,'k4':0.4,'k5':0.5,'k6':0.3,'k7':0.5,'k8':0.3,'k9':0.4,'k10':0.4,'k11':0.4,'k12':0.4,'k13':0.4,'k14':0.4,'k15':0.5}\n",
      "    print 'Finding the optimum p ... with 10 fold CV'\n",
      "    \n",
      "    #kfs = cross_validation.ShuffleSplit(X_train.shape[0], n_iter=10,test_size=.25, random_state=0)\n",
      "    \n",
      "    \n",
      "    #for y_col in train.columns[4:]:\n",
      "    #    min_error = 100\n",
      "    #    for i in linspace(0.1,1.0,num = 10):\n",
      "    #        mse_measure = []\n",
      "    #        for train_idx,cv_idx in kfs:\n",
      "    #            X_train_kf = X_train[train_idx,:]\n",
      "    #            X_cv_kf = X_train[cv_idx,:]\n",
      "    #            \n",
      "    #            y = train[y_col]\n",
      "    #            y_train_kf = y[train_idx]\n",
      "    #            y_cv_kf = y[cv_idx]\n",
      "    #            \n",
      "    #            y_train_kf = vec_y_binary_p(y_train_kf,i)\n",
      "    #            clf.fit(X_train_kf,y_train_kf.values)\n",
      "    #                           \n",
      "    #            mse_measure.append(mean_squared_error(y_cv_kf,clf.predict_proba(X_cv_kf)[:,1]))\n",
      "    #        current_error =  np.mean(mse_measure)\n",
      "    #        if current_error < min_error:\n",
      "    #            min_error = current_error\n",
      "    #            optim_p[y_col] = i\n",
      "    #        else:\n",
      "    #            print 'optimum p for ',y_col,optim_p[y_col]\n",
      "    #            break        \n",
      "    \n",
      "    print 'CV score with optimun p'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train1.shape[0],n_folds = 10)\n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf1 = X_train1[train_idx,:]\n",
      "        X_cv_kf1 = X_train1[cv_idx,:]\n",
      "        \n",
      "        X_train_kf2 = X_train2[train_idx,:]\n",
      "        X_cv_kf2 = X_train2[cv_idx,:]\n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:]\n",
      "        #creating a place holder of cv predictions\n",
      "        \n",
      "        pred_cv1 = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        pred_cv2 = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        error_pred = 100\n",
      "        \n",
      "        for y_col in train.columns[4:]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "            y_train_kf = vec_y_binary_p(y_train_kf,optim_p[y_col])\n",
      "            \n",
      "            clf.fit(X_train_kf1,y_train_kf.values)\n",
      "            pred_cv1[y_col] = clf.predict_proba(X_cv_kf1)[:,1]\n",
      "            \n",
      "            clf.fit(X_train_kf2,y_train_kf.values)\n",
      "            pred_cv2[y_col] = clf.predict_proba(X_cv_kf2)[:,1]\n",
      "            \n",
      "        pred_cv = (pred_cv1 + pred_cv2)/2    \n",
      "        error = sqrt(mean_squared_error(y_cv_kf,pred_cv))\n",
      "        final_error.append(error)\n",
      "        print 'final error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)\n",
      "    \n",
      "    \n",
      "    #for y_col in train.columns[4:]:\n",
      "    #    y = train[y_col]\n",
      "    #    y = vec_y_binary_p(y,optim_p[y_col])\n",
      "    #    \n",
      "    #    clf.fit( X_train, y.values)\n",
      "    #    #print lr.predict_proba(X_test)\n",
      "    #    pred[y_col] = clf.predict_proba(X_test)[:,1]\n",
      "    #    \n",
      "    #pred.to_csv('benchmark_sgd_optim_p.csv')\n",
      "    #print 'submission file created'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datasets ....\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finding the optimum p ... with 10 fold CV"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CV score with optimun p\n",
        "prcessing CV : 1\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.14795503252\n",
        "prcessing CV : 2\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.146729508541\n",
        "prcessing CV : 3\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.148971247248\n",
        "prcessing CV : 4\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 0.149188248584\n",
        "prcessing CV : 5\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 0.149112626476\n",
        "prcessing CV : 6\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 0.14885947314\n",
        "prcessing CV : 7\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 0.148481844908\n",
        "prcessing CV : 8\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 0.147574023625\n",
        "prcessing CV : 9\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 0.147278148627\n",
        "prcessing CV : 10\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 0.148501294398\n",
        "Mean error for 10 fold CV is  0.148265144807\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#submited , leader board score =  \t0.15610\n",
      "#Author : Chitrasen\n",
      "#Date : 11/1/2013\n",
      "# TF (no idf) , best p value, \n",
      "#combning all the classifiers\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn import cross_validation,metrics\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import linear_model as lm\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import re\n",
      "#import PorterStemmer\n",
      "\n",
      "def clean(s):\n",
      "    s = s.lower()\n",
      "    s = re.sub('n\\'t',' not',s)\n",
      "    #s = re.sub(r'\\W+', ' ', s)\n",
      "    #s = re.sub('\\s+', ' ', s)\n",
      "    #s = re.sub('\\d+','00',s)\n",
      "    return s\n",
      "\n",
      "\n",
      "def y_binary_p(y,p):\n",
      "    if y <= p:\n",
      "        return 0    \n",
      "    else:\n",
      "        return 1\n",
      "    \n",
      "def scaleCols(df):\n",
      "    s_total = df['s1']+df['s2']+df['s3']+df['s4']+df['s5']\n",
      "    df['s1'] = df['s1']/s_total\n",
      "    df['s2'] = df['s2']/s_total\n",
      "    df['s3'] = df['s3']/s_total\n",
      "    df['s4'] = df['s4']/s_total\n",
      "    df['s5'] = df['s5']/s_total\n",
      "    \n",
      "    w_total = df['w1']+df['w2']+df['w3']+df['w4']\n",
      "    df['w1'] = df['w1']/w_total\n",
      "    df['w2'] = df['w2']/w_total\n",
      "    df['w3'] = df['w3']/w_total\n",
      "    df['w4'] = df['w4']/w_total\n",
      "    \n",
      "    return df\n",
      "\n",
      "\n",
      "def main():\n",
      "    \n",
      "    print 'reading the datasets ....'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv', header = 0)\n",
      "    \n",
      "    train['tweet'] = [tweet+' '+str(state) for tweet,state in zip(train.tweet,train.state)]\n",
      "    test['tweet'] = [tweet+' '+str(state) for tweet,state in zip(test.tweet,test.state)]\n",
      "\n",
      "    pred1 = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred1.index = test['id'] \n",
      "    \n",
      "    pred2 = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred2.index = test['id']  \n",
      "    \n",
      "    pred3 = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred3.index = test['id'] \n",
      "    \n",
      "    pred4 = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred4.index = test['id']\n",
      "    \n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "\n",
      "\n",
      "    tfidf1 = TfidfVectorizer(min_df=5,max_df = .95,max_features=10000, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1,2), use_idf=False,sublinear_tf=1)\n",
      "    \n",
      "    tfidf2 = TfidfVectorizer(min_df=5,max_df = .95,max_features=10000, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1,2), use_idf=True,sublinear_tf=1)\n",
      "    \n",
      "    \n",
      "    lr = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, \n",
      "                           C=1, fit_intercept=True, intercept_scaling=1.0, \n",
      "                           class_weight=None, random_state=None)\n",
      "    \n",
      "    clf = lm.SGDClassifier(loss='modified_huber',n_iter= 100)\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "    X_all = list(tweets_train)+list(tweets_test)\n",
      "    X_all = [clean(s) for s in X_all]\n",
      "\n",
      "    tfidf1.fit(X_all)\n",
      "    X_all1 = tfidf1.transform(X_all)\n",
      "    \n",
      "    tfidf2.fit(X_all)\n",
      "    X_all2 = tfidf2.transform(X_all)\n",
      "    \n",
      "    X_train1 = X_all1[:trainLen]\n",
      "    X_test1 = X_all1[trainLen:]\n",
      "    \n",
      "    X_train2 = X_all2[:trainLen]\n",
      "    X_test2 = X_all2[trainLen:]\n",
      "    \n",
      "    vec_y_binary_p = np.vectorize(y_binary_p)\n",
      "    \n",
      "    final_cv_error = 0\n",
      "    \n",
      "    #optim_p = {}\n",
      "   \n",
      "    \n",
      "    optim_p1 = {'s1':0.3,'s2':0.5,'s3':0.4,'s4':0.4,'s5':0.4,\n",
      "         'w1':0.6,'w2':0.4,'w3':0.4,'w4':0.4,\n",
      "         'k1':0.3,'k2':0.3,'k3':0.3,'k4':0.4,'k5':0.5,'k6':0.1,'k7':0.5,'k8':0.3,'k9':0.3,'k10':0.4,\n",
      "            'k11':0.1,'k12':0.4,'k13':0.3,'k14':0.1,'k15':0.5}\n",
      "    \n",
      "    optim_p2 = {'s1':0.3,'s2':0.5,'s3':0.4,'s4':0.5,'s5':0.5,\n",
      "         'w1':0.5,'w2':0.4,'w3':0.4,'w4':0.4,\n",
      "         'k1':0.4,'k2':0.4,'k3':0.4,'k4':0.4,'k5':0.5,'k6':0.3,'k7':0.5,'k8':0.3,'k9':0.4,'k10':0.4,'k11':0.4,'k12':0.4,'k13':0.4,'k14':0.4,'k15':0.5}\n",
      "    \n",
      "    print 'Finding the optimum p ... with 10 fold CV'\n",
      "    \n",
      "    \n",
      "    print 'CV score with optimun p'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train1.shape[0],n_folds = 10)\n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf1 = X_train1[train_idx,:]\n",
      "        X_train_kf2 = X_train2[train_idx,:]\n",
      "        \n",
      "        X_cv_kf1 = X_train1[cv_idx,:]\n",
      "        X_cv_kf2 = X_train2[cv_idx,:]\n",
      "        \n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:]\n",
      "        #creating a place holder of cv predictions\n",
      "        pred_cv1 = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        pred_cv2 = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        pred_cv3 = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        pred_cv4 = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        for y_col in train.columns[4:]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "            y_train_kf1 = vec_y_binary_p(y_train_kf,optim_p1[y_col])\n",
      "            \n",
      "            y_train_kf2 = vec_y_binary_p(y_train_kf,optim_p2[y_col])\n",
      "            \n",
      "            lr.fit(X_train_kf1,y_train_kf1)\n",
      "            pred_cv1[y_col] = lr.predict_proba(X_cv_kf1)[:,1]\n",
      "            \n",
      "            lr.fit(X_train_kf2,y_train_kf1)\n",
      "            pred_cv2[y_col] = lr.predict_proba(X_cv_kf2)[:,1]        \n",
      "            #print '    error for',y_col,sqrt(mean_squared_error(y_cv_kf[y_col],(pred_cv1[y_col] + pred_cv2[y_col])/2))\n",
      "            \n",
      "            clf.fit(X_train_kf1,y_train_kf2.values)\n",
      "            pred_cv3[y_col] = clf.predict_proba(X_cv_kf1)[:,1]\n",
      "            \n",
      "            clf.fit(X_train_kf2,y_train_kf2.values)\n",
      "            pred_cv4[y_col] = clf.predict_proba(X_cv_kf2)[:,1]\n",
      "           \n",
      "        pred_cv =  0.25 * pred_cv1 + 0.25 * pred_cv2 + 0.25 * pred_cv3 + 0.25 * pred_cv4\n",
      "        error = sqrt(mean_squared_error(y_cv_kf,scaleCols(pred_cv)))\n",
      "        final_error.append(error)\n",
      "        print 'final error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)\n",
      "    \n",
      "    \n",
      "    for y_col in train.columns[4:]:\n",
      "        y = train[y_col]\n",
      "        y1 = vec_y_binary_p(y,optim_p1[y_col])\n",
      "        \n",
      "        lr.fit( X_train1, y1)        \n",
      "        pred1[y_col] = lr.predict_proba(X_test1)[:,1]\n",
      "        \n",
      "        lr.fit( X_train2, y1)        \n",
      "        pred2[y_col] = lr.predict_proba(X_test2)[:,1]\n",
      "        \n",
      "        \n",
      "        y2 = vec_y_binary_p(y,optim_p2[y_col])\n",
      "        \n",
      "        clf.fit( X_train1, y2.values)        \n",
      "        pred3[y_col] = clf.predict_proba(X_test1)[:,1]\n",
      "        \n",
      "        clf.fit( X_train2, y2.values)        \n",
      "        pred4[y_col] = clf.predict_proba(X_test2)[:,1]\n",
      "        \n",
      "    pred = scaleCols((pred1 + pred2 + pred3 + pred4)/4)    \n",
      "    pred.to_csv('benchmark_lr_sgd_ensemble_optim_p.csv')\n",
      "    print 'submission file created'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datasets ....\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finding the optimum p ... with 10 fold CV"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CV score with optimun p\n",
        "prcessing CV : 1\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.146418041609\n",
        "prcessing CV : 2\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.145131586264\n",
        "prcessing CV : 3\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.147513806913\n",
        "prcessing CV : 4\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 0.147751261262\n",
        "prcessing CV : 5\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 0.147493676981\n",
        "prcessing CV : 6\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 0.147527462368\n",
        "prcessing CV : 7\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 0.146931355607\n",
        "prcessing CV : 8\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 0.146241872713\n",
        "prcessing CV : 9\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 0.145803815031\n",
        "prcessing CV : 10\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 0.146913203245\n",
        "Mean error for 10 fold CV is  0.146772608199\n",
        "submission file created"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Author : Chitrasen\n",
      "#Date : 11/1/2013\n",
      "# Regressor \n",
      "\n",
      "import pandas as pd\n",
      "from sklearn import cross_validation,metrics\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import linear_model as lm\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "import re\n",
      "from sklearn.feature_selection import f_regression\n",
      "#import PorterStemmer\n",
      "\n",
      "def main():\n",
      "    \n",
      "    print 'reading the datasets ....'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv', header = 0)\n",
      "    \n",
      "    train['tweet'] = [tweet+' '+str(state) for tweet,state in zip(train.tweet,train.state)]\n",
      "    test['tweet'] = [tweet+' '+str(state) for tweet,state in zip(test.tweet,test.state)]\n",
      "\n",
      "    pred = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred.index = test['id']   \n",
      "    \n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "\n",
      "\n",
      "    tfidf = TfidfVectorizer(min_df=15,  max_features=15000, strip_accents='unicode', \n",
      "      analyzer='word',token_pattern=r'\\w{2,}',ngram_range=(1, 2), use_idf=False,sublinear_tf=1,binary=True)\n",
      "    \n",
      "    #regr = lm.LassoLars(alpha=.1)\n",
      "    regr = lm.Ridge(alpha = 1)\n",
      "\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "    X_all = list(tweets_train)+list(tweets_test)\n",
      "    #X_all = [clean(s) for s in X_all_a]   \n",
      "        \n",
      "\n",
      "    tfidf.fit(X_all)\n",
      "    X_all = tfidf.transform(X_all)   \n",
      "\n",
      "    \n",
      "    X_train = X_all[:trainLen]\n",
      "    X_test = X_all[trainLen:]\n",
      "    \n",
      "    vec_y_binary_p = np.vectorize(y_binary_p)\n",
      "    \n",
      "    final_cv_error = 0\n",
      "    \n",
      "    print 'CV score with optimun p'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf = X_train[train_idx,:]\n",
      "        X_cv_kf = X_train[cv_idx,:]\n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:]\n",
      "        #creating a place holder of cv predictions        \n",
      "        pred_cv = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:]) \n",
      "        \n",
      "        for y_col in train.columns[4:]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "            #y_train_kf = vec_y_binary_p(y_train_kf,optim_p[y_col])\n",
      "            \n",
      "            #f,p = f_regression(X_train_kf,y_train_kf,center=False)\n",
      "            \n",
      "            regr.fit(X_train_kf,log(y_train_kf+1))\n",
      "            pred_cv[y_col] = exp(regr.predict(X_cv_kf)) -1\n",
      "            #print '    error for',y_col,sqrt(mean_squared_error(y_cv_kf[y_col],pred_cv[y_col]))\n",
      "        \n",
      "        #regr.fit(X_train[train_idx,:],train.ix[train_idx,4:])\n",
      "        #pred_cv = regr.predict(X_train[cv_idx,:])\n",
      "        error = sqrt(mean_squared_error(y_cv_kf,pred_cv))\n",
      "        final_error.append(error)\n",
      "        print 'final error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)\n",
      "    \n",
      "    \n",
      "    #for y_col in train.columns[4:]:\n",
      "    #    y = train[y_col]\n",
      "    #    y = vec_y_binary_p(y,optim_p[y_col])\n",
      "    #    \n",
      "    #    lr.fit( X_train, y)\n",
      "    #    #print lr.predict_proba(X_test)\n",
      "    #    pred[y_col] = lr.predict_proba(X_test)[:,1]\n",
      "        \n",
      "    #pred.to_csv('benchmark_lr_optim_p.csv')\n",
      "    #print 'submission file created'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datasets ....\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CV score with optimun p"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "prcessing CV : 1\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.155653679386\n",
        "prcessing CV : 2\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.153851675925\n",
        "prcessing CV : 3\n",
        "final error for cv"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-78-28be1e37e7b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-78-28be1e37e7b7>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;31m#f,p = f_regression(X_train_kf,y_train_kf,center=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_kf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_kf\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[0mpred_cv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_col\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_cv_kf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;31m#print '    error for',y_col,sqrt(mean_squared_error(y_cv_kf[y_col],pred_cv[y_col]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \"\"\"\n\u001b[1;32m--> 449\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRidge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    336\u001b[0m                                       \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m                                       \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m                                       solver=self.solver)\n\u001b[0m\u001b[0;32m    339\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_intercept\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36mridge_regression\u001b[1;34m(X, y, alpha, sample_weight, solver, max_iter, tol)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sparse_cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         \u001b[0mcoef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_solve_sparse_cg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"lsqr\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36m_solve_sparse_cg\u001b[1;34m(X, y, alpha, max_iter, tol)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 (n_features, n_features), matvec=mv, dtype=X.dtype)\n\u001b[0;32m     64\u001b[0m             coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter,\n\u001b[1;32m---> 65\u001b[1;33m                                           tol=tol)\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Failed with error code %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/linalg/isolve/iterative.pyc\u001b[0m in \u001b[0;36mcg\u001b[1;34m(A, b, x0, tol, maxiter, xtype, M, callback)\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/linalg/isolve/iterative.pyc\u001b[0m in \u001b[0;36mnon_reentrant\u001b[1;34m(func, *a, **kw)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__entered'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__entered'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/linalg/isolve/iterative.pyc\u001b[0m in \u001b[0;36mcg\u001b[1;34m(A, b, x0, tol, maxiter, xtype, M, callback)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mijob\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[0mwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0msclr2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m             \u001b[0mwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msclr1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmatvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mijob\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0mwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpsolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/linalg/interface.pyc\u001b[0m in \u001b[0;36mmatvec\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36m_mv\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcreate_mv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_alpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0m_mv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmatvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcurr_alpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/linalg/interface.pyc\u001b[0m in \u001b[0;36mrmatvec\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA_conj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA_conj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA_conj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36mdot\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \"\"\"\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[1;31m# Fast path for the most common case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36m_mul_vector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;31m# csr_matvec or csc_matvec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparsetools\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_matvec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/sparsetools/csc.pyc\u001b[0m in \u001b[0;36mcsc_matvec\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    229\u001b[0m         npy_clongdouble_wrapper const [] Xx, npy_clongdouble_wrapper [] Yx)\n\u001b[0;32m    230\u001b[0m     \"\"\"\n\u001b[1;32m--> 231\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_csc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsc_matvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcsc_matvecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.155651715882\n",
        "prcessing CV : 4\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#submission pending\n",
      "#Author : Chitrasen\n",
      "#Date : 11/1/2013\n",
      "# TF (no idf) , best p value, \n",
      "#combning all the classifiers\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn import cross_validation,metrics\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import linear_model as lm\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import re\n",
      "#import PorterStemmer\n",
      "\n",
      "def clean(s):\n",
      "    s = s.lower()\n",
      "    s = re.sub('n\\'t',' not',s)\n",
      "    #s = re.sub(r'\\W+', ' ', s)\n",
      "    #s = re.sub('\\s+', ' ', s)\n",
      "    #s = re.sub('\\d+','00',s)\n",
      "    return s\n",
      "\n",
      "\n",
      "def y_binary_p(y,p):\n",
      "    if y <= p:\n",
      "        return 0    \n",
      "    else:\n",
      "        return 1\n",
      "\n",
      "\n",
      "def main():\n",
      "    \n",
      "    print 'reading the datasets ....'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv', header = 0)\n",
      "    \n",
      "    train['tweet'] = [tweet+' '+str(state) for tweet,state in zip(train.tweet,train.state)]\n",
      "    test['tweet'] = [tweet+' '+str(state) for tweet,state in zip(test.tweet,test.state)]\n",
      "\n",
      "    pred1 = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred1.index = test['id'] \n",
      "    \n",
      "    pred2 = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred2.index = test['id']  \n",
      "    \n",
      "    pred3 = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred3.index = test['id'] \n",
      "    \n",
      "    pred4 = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred4.index = test['id']\n",
      "    \n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "\n",
      "\n",
      "    tfidf1 = TfidfVectorizer(min_df=5,max_df = .95,max_features=10000, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1,2), use_idf=False,sublinear_tf=1)\n",
      "    \n",
      "    tfidf2 = TfidfVectorizer(min_df=5,max_df = .95,max_features=10000, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1,2), use_idf=True,sublinear_tf=1)\n",
      "    \n",
      "    \n",
      "    lr = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, \n",
      "                           C=1, fit_intercept=True, intercept_scaling=1.0, \n",
      "                           class_weight=None, random_state=None)\n",
      "    \n",
      "    clf = lm.SGDClassifier(loss='modified_huber',n_iter= 100)\n",
      "    \n",
      "    regr = lm.Ridge(alpha = 1)\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "    X_all = list(tweets_train)+list(tweets_test)\n",
      "    X_all = [clean(s) for s in X_all]\n",
      "\n",
      "    tfidf1.fit(X_all)\n",
      "    X_all1 = tfidf1.transform(X_all)\n",
      "    \n",
      "    tfidf2.fit(X_all)\n",
      "    X_all2 = tfidf2.transform(X_all)\n",
      "    \n",
      "    X_train1 = X_all1[:trainLen]\n",
      "    X_test1 = X_all1[trainLen:]\n",
      "    \n",
      "    X_train2 = X_all2[:trainLen]\n",
      "    X_test2 = X_all2[trainLen:]\n",
      "    \n",
      "    vec_y_binary_p = np.vectorize(y_binary_p)\n",
      "    \n",
      "    final_cv_error = 0\n",
      "    \n",
      "    #optim_p = {}\n",
      "   \n",
      "    \n",
      "    optim_p1 = {'s1':0.3,'s2':0.5,'s3':0.4,'s4':0.4,'s5':0.4,\n",
      "         'w1':0.6,'w2':0.4,'w3':0.4,'w4':0.4,\n",
      "         'k1':0.3,'k2':0.3,'k3':0.3,'k4':0.4,'k5':0.5,'k6':0.1,'k7':0.5,'k8':0.3,'k9':0.3,'k10':0.4,\n",
      "            'k11':0.1,'k12':0.4,'k13':0.3,'k14':0.1,'k15':0.5}\n",
      "    \n",
      "    optim_p2 = {'s1':0.3,'s2':0.5,'s3':0.4,'s4':0.5,'s5':0.5,\n",
      "         'w1':0.5,'w2':0.4,'w3':0.4,'w4':0.4,\n",
      "         'k1':0.4,'k2':0.4,'k3':0.4,'k4':0.4,'k5':0.5,'k6':0.3,'k7':0.5,'k8':0.3,'k9':0.4,'k10':0.4,'k11':0.4,'k12':0.4,'k13':0.4,'k14':0.4,'k15':0.5}\n",
      "    \n",
      "    print 'Finding the optimum p ... with 10 fold CV'\n",
      "    \n",
      "    \n",
      "    print 'CV score with optimun p'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train1.shape[0],n_folds = 10)\n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf1 = X_train1[train_idx,:]\n",
      "        X_train_kf2 = X_train2[train_idx,:]\n",
      "        \n",
      "        X_cv_kf1 = X_train1[cv_idx,:]\n",
      "        X_cv_kf2 = X_train2[cv_idx,:]\n",
      "        \n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:]\n",
      "        #creating a place holder of cv predictions\n",
      "        pred_cv1 = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        pred_cv2 = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        pred_cv3 = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        pred_cv4 = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        pred_cv5 = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:])\n",
      "        \n",
      "        for y_col in train.columns[4:]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "            y_train_kf1 = vec_y_binary_p(y_train_kf,optim_p1[y_col])\n",
      "            \n",
      "            y_train_kf2 = vec_y_binary_p(y_train_kf,optim_p2[y_col])\n",
      "            \n",
      "            lr.fit(X_train_kf1,y_train_kf1)\n",
      "            pred_cv1[y_col] = lr.predict_proba(X_cv_kf1)[:,1]\n",
      "            \n",
      "            lr.fit(X_train_kf2,y_train_kf1)\n",
      "            pred_cv2[y_col] = lr.predict_proba(X_cv_kf2)[:,1]        \n",
      "            #print '    error for',y_col,sqrt(mean_squared_error(y_cv_kf[y_col],(pred_cv1[y_col] + pred_cv2[y_col])/2))\n",
      "            \n",
      "            clf.fit(X_train_kf1,y_train_kf2.values)\n",
      "            pred_cv3[y_col] = clf.predict_proba(X_cv_kf1)[:,1]\n",
      "            \n",
      "            clf.fit(X_train_kf2,y_train_kf2.values)\n",
      "            pred_cv4[y_col] = clf.predict_proba(X_cv_kf2)[:,1]\n",
      "            \n",
      "            regr.fit(X_train_kf1,y_train_kf1)\n",
      "            pred_cv5[y_col] = regr.predict(X_cv_kf1)\n",
      "           \n",
      "        pred_cv =  0.6 * (0.25 * pred_cv1 + 0.25 * pred_cv2 + 0.25 * pred_cv3 + 0.25 * pred_cv4) + (0.4 * pred_cv5)\n",
      "        error = sqrt(mean_squared_error(y_cv_kf,pred_cv))\n",
      "        final_error.append(error)\n",
      "        print 'final error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)\n",
      "    \n",
      "    \n",
      "    #for y_col in train.columns[4:]:\n",
      "    #    y = train[y_col]\n",
      "    #    y1 = vec_y_binary_p(y,optim_p1[y_col])\n",
      "    #    \n",
      "    #    lr.fit( X_train1, y1)        \n",
      "    #    pred1[y_col] = lr.predict_proba(X_test1)[:,1]\n",
      "    #    \n",
      "    #    lr.fit( X_train2, y1)        \n",
      "    #    pred2[y_col] = lr.predict_proba(X_test2)[:,1]\n",
      "    #    \n",
      "    #    \n",
      "    #    y2 = vec_y_binary_p(y,optim_p2[y_col])\n",
      "    #    \n",
      "    #    clf.fit( X_train1, y2.values)        \n",
      "    #    pred3[y_col] = clf.predict_proba(X_test1)[:,1]\n",
      "    #    \n",
      "    #    clf.fit( X_train2, y2.values)        \n",
      "    #    pred4[y_col] = clf.predict_proba(X_test2)[:,1]\n",
      "    #    \n",
      "    #pred = (pred1 + pred2 + pred3 + pred4)/4\n",
      "    #pred.to_csv('benchmark_lr_sgd_ensemble_optim_p.csv')\n",
      "    #print 'submission file created'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datasets ....\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finding the optimum p ... with 10 fold CV"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CV score with optimun p\n",
        "prcessing CV : 1\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.148944836831\n",
        "prcessing CV : 2\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.147813654748\n",
        "prcessing CV : 3\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.150016908465\n",
        "prcessing CV : 4\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 0.150281803345\n",
        "prcessing CV : 5\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 0.150086636135\n",
        "prcessing CV : 6\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 0.149902584766\n",
        "prcessing CV : 7\n",
        "final error for cv"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-59-2c8cd6e7aa1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-59-2c8cd6e7aa1d>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0my_train_kf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec_y_binary_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_kf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptim_p2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_kf1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_kf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m             \u001b[0mpred_cv1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_col\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_cv_kf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    690\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                                               rnd.randint(np.iinfo('i').max))\n\u001b[0m\u001b[0;32m    693\u001b[0m         \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 0.149509014695\n",
        "prcessing CV : 8\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Author : Chitrasen\n",
      "#Date : 11/1/2013\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn import cross_validation,metrics\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import linear_model as lm\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "import re\n",
      "#import PorterStemmer\n",
      "\n",
      "def main():\n",
      "    \n",
      "    print 'reading the datasets ....'\n",
      "    train = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/train.csv',header = 0)\n",
      "    test = pd.read_csv('/home/trupti/data_local/kaggle/partly_sunny/test.csv', header = 0)\n",
      "    \n",
      "    train['tweet'] = [tweet+' '+str(state) for tweet,state in zip(train.tweet,train.state)]\n",
      "    test['tweet'] = [tweet+' '+str(state) for tweet,state in zip(test.tweet,test.state)]\n",
      "\n",
      "    pred = pd.DataFrame(np.zeros((test.shape[0], train.shape[1]- test.shape[1])), columns = train.columns[4:])\n",
      "    pred.index = test['id']   \n",
      "    \n",
      "    \n",
      "    tweets_train = train['tweet']\n",
      "    tweets_test = test['tweet']\n",
      "\n",
      "\n",
      "    tfidf = TfidfVectorizer(min_df=5,  max_features=10000, strip_accents='unicode',  \n",
      "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=True,sublinear_tf=1)\n",
      "    \n",
      "    #regr = lm.LassoLars(alpha=.1)\n",
      "    clf = lm.SGDClassifier(loss='modified_huber',n_iter = 150)\n",
      "    regr = lm.SGDRegressor(loss = 'squared_loss',penalty = 'l2',n_iter = 100)\n",
      "\n",
      "    \n",
      "    print 'fitting TfIDF...'\n",
      "    trainLen = tweets_train.shape[0]\n",
      "    X_all = list(tweets_train)+list(tweets_test)\n",
      "    #X_all = [clean(s) for s in X_all_a]   \n",
      "        \n",
      "\n",
      "    tfidf.fit(X_all)\n",
      "    X_all = tfidf.transform(X_all)\n",
      "        \n",
      "    X_train = X_all[:trainLen]\n",
      "    X_test = X_all[trainLen:]\n",
      "    \n",
      "    vec_y_binary_p = np.vectorize(y_binary_p)\n",
      "    \n",
      "    final_cv_error = 0\n",
      "    \n",
      "    print 'CV score with optimun p'\n",
      "    \n",
      "    kf = cross_validation.KFold(X_train.shape[0],n_folds = 10)\n",
      "    \n",
      "    final_error = []\n",
      "    j = 0\n",
      "    for train_idx,cv_idx in kf: \n",
      "        j +=1\n",
      "        print 'prcessing CV :',j\n",
      "        X_train_kf = X_train[train_idx,:]\n",
      "        X_cv_kf = X_train[cv_idx,:]\n",
      "        \n",
      "        y_cv_kf = train.ix[cv_idx,4:]\n",
      "        #creating a place holder of cv predictions        \n",
      "        pred_cv = pd.DataFrame(np.zeros(y_cv_kf.shape), columns = train.columns[4:]) \n",
      "        \n",
      "        for y_col in train.columns[4:]:\n",
      "            y = train[y_col]            \n",
      "            y_train_kf = y[train_idx]            \n",
      "            #y_train_kf = vec_y_binary_p(y_train_kf,optim_p[y_col])\n",
      "            \n",
      "            regr.fit(X_train_kf,y_train_kf)\n",
      "            pred_cv[y_col] = regr.predict(X_cv_kf)\n",
      "            #print '    error for',y_col,sqrt(mean_squared_error(y_cv_kf[y_col],pred_cv[y_col]))\n",
      "        error = sqrt(mean_squared_error(y_cv_kf,pred_cv))\n",
      "        final_error.append(error)\n",
      "        print 'final error for cv',j, error\n",
      "    \n",
      "    print 'Mean error for 10 fold CV is ',np.mean(final_error)\n",
      "    \n",
      "    \n",
      "    #for y_col in train.columns[4:]:\n",
      "    #    y = train[y_col]\n",
      "    #    y = vec_y_binary_p(y,optim_p[y_col])\n",
      "    #    \n",
      "    #    lr.fit( X_train, y)\n",
      "    #    #print lr.predict_proba(X_test)\n",
      "    #    pred[y_col] = lr.predict_proba(X_test)[:,1]\n",
      "        \n",
      "    #pred.to_csv('benchmark_lr_optim_p.csv')\n",
      "    #print 'submission file created'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading the datasets ....\n",
        "fitting TfIDF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CV score with optimun p"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "prcessing CV : 1\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.164154405074\n",
        "prcessing CV : 2\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.162572185149\n",
        "prcessing CV : 3\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.164158980252\n",
        "prcessing CV : 4\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 0.164729740093\n",
        "prcessing CV : 5\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 0.164118534289\n",
        "prcessing CV : 6\n",
        "final error for cv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 0.164357450607\n",
        "prcessing CV : 7\n",
        "final error for cv"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-35-fc0a6fccea3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-35-fc0a6fccea3d>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;31m#y_train_kf = vec_y_binary_p(y_train_kf,optim_p[y_col])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_kf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_kf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mpred_cv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_col\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_cv_kf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;31m#print '    error for',y_col,sqrt(mean_squared_error(y_cv_kf[y_col],pred_cv[y_col]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    888\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m                          \u001b[0mintercept_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m                          sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    857\u001b[0m         return self._partial_fit(X, y, alpha, C, loss, learning_rate,\n\u001b[0;32m    858\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                                  coef_init, intercept_init)\n\u001b[0m\u001b[0;32m    860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m     def fit(self, X, y, coef_init=None, intercept_init=None,\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_partial_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, n_iter, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m         self._fit_regressor(X, y, alpha, C, loss, learning_rate,\n\u001b[1;32m--> 812\u001b[1;33m                             sample_weight, n_iter)\n\u001b[0m\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_fit_regressor\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, sample_weight, n_iter)\u001b[0m\n\u001b[0;32m    946\u001b[0m                                           \u001b[0mlearning_rate_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meta0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m                                           intercept_decay)\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintercept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/sgd_fast.so\u001b[0m in \u001b[0;36msklearn.linear_model.sgd_fast.plain_sgd (sklearn/linear_model/sgd_fast.c:8527)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36many\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1760\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1762\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1763\u001b[0m     \"\"\"\n\u001b[0;32m   1764\u001b[0m     \u001b[0mTest\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0many\u001b[0m \u001b[0marray\u001b[0m \u001b[0melement\u001b[0m \u001b[0malong\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0maxis\u001b[0m \u001b[0mevaluates\u001b[0m \u001b[0mto\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 0.163705444964\n",
        "prcessing CV : 8\n"
       ]
      }
     ],
     "prompt_number": 35
    }
   ],
   "metadata": {}
  }
 ]
}